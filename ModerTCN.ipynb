{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. data_provider/dataloader.py\n",
    "\n",
    "Trying to import the UAEloader dataset which was used for time series classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.timefeatures'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtimefeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time_features\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_provider\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mm4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m M4Dataset, M4Meta\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_provider\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muea\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m subsample, interpolate_missing, Normalizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.timefeatures'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.timefeatures import time_features\n",
    "from data_provider.m4 import M4Dataset, M4Meta\n",
    "from data_provider.uea import subsample, interpolate_missing, Normalizer\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data_provider/dataloader.py ------> utils/timefeatures.py\n",
    "\n",
    "Trying to import time_features function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. data_provider/dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UEAloader(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for datasets included in:\n",
    "        Time Series Classification Archive (www.timeseriesclassification.com)\n",
    "    Argument:\n",
    "        limit_size: float in (0, 1) for debug\n",
    "    Attributes:\n",
    "        all_df: (num_samples * seq_len, num_columns) dataframe indexed by integer indices, with multiple rows corresponding to the same index (sample).\n",
    "            Each row is a time step; Each column contains either metadata (e.g. timestamp) or a feature.\n",
    "        feature_df: (num_samples * seq_len, feat_dim) dataframe; contains the subset of columns of `all_df` which correspond to selected features\n",
    "        feature_names: names of columns contained in `feature_df` (same as feature_df.columns)\n",
    "        all_IDs: (num_samples,) series of IDs contained in `all_df`/`feature_df` (same as all_df.index.unique() )\n",
    "        labels_df: (num_samples, num_labels) pd.DataFrame of label(s) for each sample\n",
    "        max_seq_len: maximum sequence (time series) length. If None, script argument `max_seq_len` will be used.\n",
    "            (Moreover, script argument overrides this attribute)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_path, file_list=None, limit_size=None, flag=None):\n",
    "        self.root_path = root_path\n",
    "        self.all_df, self.labels_df = self.load_all(root_path, file_list=file_list, flag=flag)\n",
    "        self.all_IDs = self.all_df.index.unique()  # all sample IDs (integer indices 0 ... num_samples-1)\n",
    "\n",
    "        if limit_size is not None:\n",
    "            if limit_size > 1:\n",
    "                limit_size = int(limit_size)\n",
    "            else:  # interpret as proportion if in (0, 1]\n",
    "                limit_size = int(limit_size * len(self.all_IDs))\n",
    "            self.all_IDs = self.all_IDs[:limit_size]\n",
    "            self.all_df = self.all_df.loc[self.all_IDs]\n",
    "\n",
    "        # use all features\n",
    "        self.feature_names = self.all_df.columns\n",
    "        self.feature_df = self.all_df\n",
    "\n",
    "        # pre_process\n",
    "        normalizer = Normalizer()\n",
    "        self.feature_df = normalizer.normalize(self.feature_df)\n",
    "        print(len(self.all_IDs))\n",
    "\n",
    "    def load_all(self, root_path, file_list=None, flag=None):\n",
    "        \"\"\"\n",
    "        Loads datasets from csv files contained in `root_path` into a dataframe, optionally choosing from `pattern`\n",
    "        Args:\n",
    "            root_path: directory containing all individual .csv files\n",
    "            file_list: optionally, provide a list of file paths within `root_path` to consider.\n",
    "                Otherwise, entire `root_path` contents will be used.\n",
    "        Returns:\n",
    "            all_df: a single (possibly concatenated) dataframe with all data corresponding to specified files\n",
    "            labels_df: dataframe containing label(s) for each sample\n",
    "        \"\"\"\n",
    "        # Select paths for training and evaluation\n",
    "        if file_list is None:\n",
    "            data_paths = glob.glob(os.path.join(root_path, '*'))  # list of all paths\n",
    "        else:\n",
    "            data_paths = [os.path.join(root_path, p) for p in file_list]\n",
    "        if len(data_paths) == 0:\n",
    "            raise Exception('No files found using: {}'.format(os.path.join(root_path, '*')))\n",
    "        if flag is not None:\n",
    "            data_paths = list(filter(lambda x: re.search(flag, x), data_paths))\n",
    "        input_paths = [p for p in data_paths if os.path.isfile(p) and p.endswith('.ts')]\n",
    "        if len(input_paths) == 0:\n",
    "            raise Exception(\"No .ts files found using pattern: '{}'\".format(pattern))\n",
    "\n",
    "        all_df, labels_df = self.load_single(input_paths[0])  # a single file contains dataset\n",
    "\n",
    "        return all_df, labels_df\n",
    "\n",
    "    def load_single(self, filepath):\n",
    "        df, labels = load_from_tsfile_to_dataframe(filepath, return_separate_X_and_y=True,\n",
    "                                                             replace_missing_vals_with='NaN')\n",
    "        labels = pd.Series(labels, dtype=\"category\")\n",
    "        self.class_names = labels.cat.categories\n",
    "        labels_df = pd.DataFrame(labels.cat.codes,\n",
    "                                 dtype=np.int8)  # int8-32 gives an error when using nn.CrossEntropyLoss\n",
    "\n",
    "        lengths = df.applymap(\n",
    "            lambda x: len(x)).values  # (num_samples, num_dimensions) array containing the length of each series\n",
    "\n",
    "        horiz_diffs = np.abs(lengths - np.expand_dims(lengths[:, 0], -1))\n",
    "\n",
    "        if np.sum(horiz_diffs) > 0:  # if any row (sample) has varying length across dimensions\n",
    "            df = df.applymap(subsample)\n",
    "\n",
    "        lengths = df.applymap(lambda x: len(x)).values\n",
    "        vert_diffs = np.abs(lengths - np.expand_dims(lengths[0, :], 0))\n",
    "        if np.sum(vert_diffs) > 0:  # if any column (dimension) has varying length across samples\n",
    "            self.max_seq_len = int(np.max(lengths[:, 0]))\n",
    "        else:\n",
    "            self.max_seq_len = lengths[0, 0]\n",
    "\n",
    "        # First create a (seq_len, feat_dim) dataframe for each sample, indexed by a single integer (\"ID\" of the sample)\n",
    "        # Then concatenate into a (num_samples * seq_len, feat_dim) dataframe, with multiple rows corresponding to the\n",
    "        # sample index (i.e. the same scheme as all datasets in this project)\n",
    "\n",
    "        df = pd.concat((pd.DataFrame({col: df.loc[row, col] for col in df.columns}).reset_index(drop=True).set_index(\n",
    "            pd.Series(lengths[row, 0] * [row])) for row in range(df.shape[0])), axis=0)\n",
    "\n",
    "        # Replace NaN values\n",
    "        grp = df.groupby(by=df.index)\n",
    "        df = grp.transform(interpolate_missing)\n",
    "\n",
    "        return df, labels_df\n",
    "\n",
    "    def instance_norm(self, case):\n",
    "        if self.root_path.count('EthanolConcentration') > 0:  # special process for numerical stability\n",
    "            mean = case.mean(0, keepdim=True)\n",
    "            case = case - mean\n",
    "            stdev = torch.sqrt(torch.var(case, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            case /= stdev\n",
    "            return case\n",
    "        else:\n",
    "            return case\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return self.instance_norm(torch.from_numpy(self.feature_df.loc[self.all_IDs[ind]].values)), \\\n",
    "               torch.from_numpy(self.labels_df.loc[self.all_IDs[ind]].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_IDs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
