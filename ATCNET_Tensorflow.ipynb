{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, AveragePooling2D, MaxPooling2D\n",
    "from keras.layers import Conv1D, Conv2D, SeparableConv2D, DepthwiseConv2D, MultiHeadAttention\n",
    "from keras.layers import BatchNormalization, LayerNormalization, Flatten\n",
    "from keras.layers import Add, multiply, Concatenate, Lambda, Input, Permute, Reshape\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.regularizers import L2 \n",
    "from keras.constraints import max_norm\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Conv_block (Models.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv_block_(input_layer, F1=4, kernLength=64, poolSize=8, D=2, in_chans=22, \n",
    "                weightDecay = 0.009, maxNorm = 0.6, dropout=0.25):\n",
    "    \"\"\" Conv_block\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "        using  different regularization methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    F2= F1*D\n",
    "    block1 = Conv2D(F1, (kernLength, 1), padding = 'same', data_format='channels_last', \n",
    "                    kernel_regularizer=L2(weightDecay),\n",
    "                    \n",
    "                    # In a Conv2D layer with data_format=\"channels_last\", the weight tensor has shape \n",
    "                    # (rows, cols, input_depth, output_depth), set axis to [0, 1, 2] to constrain \n",
    "                    # the weights of each filter tensor of size (rows, cols, input_depth).\n",
    "                    kernel_constraint = max_norm(maxNorm, axis=[0,1,2]),\n",
    "                    use_bias = False)(input_layer)\n",
    "    block1 = BatchNormalization(axis = -1)(block1)  # bn_axis = -1 if data_format() == 'channels_last' else 1\n",
    "    \n",
    "    block2 = DepthwiseConv2D((1, in_chans),  \n",
    "                             depth_multiplier = D,\n",
    "                             data_format='channels_last',\n",
    "                             depthwise_regularizer=L2(weightDecay),\n",
    "                             depthwise_constraint  = max_norm(maxNorm, axis=[0,1,2]),\n",
    "                             use_bias = False)(block1)\n",
    "    block2 = BatchNormalization(axis = -1)(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "    block2 = AveragePooling2D((8,1),data_format='channels_last')(block2)\n",
    "    block2 = Dropout(dropout)(block2)\n",
    "    \n",
    "    block3 = Conv2D(F2, (16, 1),\n",
    "                            data_format='channels_last',\n",
    "                            kernel_regularizer=L2(weightDecay),\n",
    "                            kernel_constraint = max_norm(maxNorm, axis=[0,1,2]),\n",
    "                            use_bias = False, padding = 'same')(block2)\n",
    "    block3 = BatchNormalization(axis = -1)(block3)\n",
    "    block3 = Activation('elu')(block3)\n",
    "    \n",
    "    block3 = AveragePooling2D((poolSize,1),data_format='channels_last')(block3)\n",
    "    block3 = Dropout(dropout)(block3)\n",
    "    return block3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. attention_block (Model.py  ------>  attention_models.py)\n",
    "\n",
    "attention_block takes dependancies from attention_module.py. This has three types of attention mechanisms used.\n",
    "1. Multihead Self Attention - mha_block\n",
    "2. Squeeze and Excitation Attention - se_block\n",
    "3. Covolutional Block Attention - cbam_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. mha_block. Best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_LSA(tf.keras.layers.MultiHeadAttention):\n",
    "    \"\"\"local multi-head self attention block\n",
    "     \n",
    "     Locality Self Attention as described in https://arxiv.org/abs/2112.13492v1\n",
    "     This implementation is taken from  https://keras.io/examples/vision/vit_small_ds/ \n",
    "    \"\"\"    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # The trainable temperature term. The initial value is the square \n",
    "        # root of the key dimension.\n",
    "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n",
    "\n",
    "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
    "        query = tf.multiply(query, 1.0 / self.tau)\n",
    "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "        attention_scores_dropout = self._dropout_layer(\n",
    "            attention_scores, training=training\n",
    "        )\n",
    "        attention_output = tf.einsum(\n",
    "            self._combine_equation, attention_scores_dropout, value\n",
    "        )\n",
    "        return attention_output, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mha_block(input_feature, key_dim=8, num_heads=2, dropout = 0.5, vanilla = True):\n",
    "    \"\"\"Multi Head self Attention (MHA) block.     \n",
    "       \n",
    "    Here we include two types of MHA blocks: \n",
    "            The original multi-head self-attention as described in https://arxiv.org/abs/1706.03762\n",
    "            The multi-head local self attention as described in https://arxiv.org/abs/2112.13492v1\n",
    "    \"\"\"    \n",
    "    # Layer normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(input_feature)\n",
    "    \n",
    "    if vanilla:\n",
    "        # Create a multi-head attention layer as described in \n",
    "        # 'Attention Is All You Need' https://arxiv.org/abs/1706.03762\n",
    "        x = MultiHeadAttention(key_dim = key_dim, num_heads = num_heads, dropout = dropout)(x, x)\n",
    "    else:\n",
    "        # Create a multi-head local self-attention layer as described in \n",
    "        # 'Vision Transformer for Small-Size Datasets' https://arxiv.org/abs/2112.13492v1\n",
    "        \n",
    "        # Build the diagonal attention mask\n",
    "        NUM_PATCHES = input_feature.shape[1]\n",
    "        diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
    "        diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)\n",
    "        \n",
    "        # Create a multi-head local self attention layer.\n",
    "        # x = MultiHeadAttention_LSA(key_dim = key_dim, num_heads = num_heads, dropout = dropout)(\n",
    "        #     x, x, attention_mask = diag_attn_mask)\n",
    "        x = MultiHeadAttention_LSA(key_dim = key_dim, num_heads = num_heads, dropout = dropout)(\n",
    "            x, x, attention_mask = diag_attn_mask)\n",
    "    x = Dropout(0.3)(x)\n",
    "    # Skip connection\n",
    "    mha_feature = Add()([input_feature, x])\n",
    "    \n",
    "    return mha_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. se_block (Squeeze and Excitation Block). Not need to be replicated since the best performance is from mha_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(input_feature, ratio=8, residual = False, apply_to_input=True):\n",
    "    \"\"\"Squeeze-and-Excitation(SE) block.\n",
    "    \n",
    "    As described in https://arxiv.org/abs/1709.01507\n",
    "    The implementation is taken from https://github.com/kobiso/CBAM-keras\n",
    "    \"\"\"\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature.shape[channel_axis]\n",
    "\n",
    "    se_feature = GlobalAveragePooling2D()(input_feature)\n",
    "    se_feature = Reshape((1, 1, channel))(se_feature)\n",
    "    assert se_feature.shape[1:] == (1,1,channel)\n",
    "    if (ratio != 0):\n",
    "        se_feature = Dense(channel // ratio,\n",
    "                           activation='relu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           use_bias=True,\n",
    "                           bias_initializer='zeros')(se_feature)\n",
    "        assert se_feature.shape[1:] == (1,1,channel//ratio)\n",
    "    se_feature = Dense(channel,\n",
    "                       activation='sigmoid',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       use_bias=True,\n",
    "                       bias_initializer='zeros')(se_feature)\n",
    "    assert se_feature.shape[1:] == (1,1,channel)\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        se_feature = Permute((3, 1, 2))(se_feature)\n",
    "        \n",
    "    if(apply_to_input):\n",
    "        se_feature = multiply([input_feature, se_feature])\n",
    "    \n",
    "    # Residual Connection\n",
    "    if(residual): \n",
    "        se_feature = Add()([se_feature, input_feature])\n",
    "\n",
    "    return se_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. cbam_block (Convlusional Block Attention Module). Not need to be replicated since the best performance is from mha_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_attention(input_feature):\n",
    "    kernel_size = 7\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature.shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature.shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "    \n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool.shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool.shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat.shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters = 1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat)    \n",
    "    assert cbam_feature.shape[-1] == 1\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "        \n",
    "    return multiply([input_feature, cbam_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_attention(input_feature, ratio=8):\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "#     channel = input_feature._keras_shape[channel_axis]\n",
    "    channel = input_feature.shape[channel_axis]\n",
    "    \n",
    "    shared_layer_one = Dense(channel//ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = Reshape((1,1,channel))(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel)\n",
    "    \n",
    "    cbam_feature = Add()([avg_pool,max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "    \n",
    "    return multiply([input_feature, cbam_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbam_block(input_feature, ratio=8, residual = False):\n",
    "    \"\"\" Convolutional Block Attention Module(CBAM) block.\n",
    "    \n",
    "    As described in https://arxiv.org/abs/1807.06521\n",
    "    The implementation is taken from https://github.com/kobiso/CBAM-keras\n",
    "    \"\"\"\n",
    "    \n",
    "    cbam_feature = channel_attention(input_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "    \n",
    "    # Residual Connection\n",
    "    if(residual): \n",
    "        cbam_feature = Add()([input_feature, cbam_feature])\n",
    "\n",
    "    return cbam_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. Attention_block that combines all three attention mechanisms into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(in_layer, attention_model, ratio=8, residual = False, apply_to_input=True): \n",
    "    in_sh = in_layer.shape # dimensions of the input tensor\n",
    "    in_len = len(in_sh) \n",
    "    expanded_axis = 2 # defualt = 2\n",
    "    \n",
    "    if attention_model == 'mha':   # Multi-head self attention layer \n",
    "        if(in_len > 3):\n",
    "            in_layer = Reshape((in_sh[1],-1))(in_layer)\n",
    "        out_layer = mha_block(in_layer)\n",
    "    elif attention_model == 'mhla':  # Multi-head local self-attention layer \n",
    "        if(in_len > 3):\n",
    "            in_layer = Reshape((in_sh[1],-1))(in_layer)\n",
    "        out_layer = mha_block(in_layer, vanilla = False)\n",
    "    elif attention_model == 'se':   # Squeeze-and-excitation layer\n",
    "        if(in_len < 4):\n",
    "            in_layer = tf.expand_dims(in_layer, axis=expanded_axis)\n",
    "        out_layer = se_block(in_layer, ratio, residual, apply_to_input)\n",
    "    elif attention_model == 'cbam': # Convolutional block attention module\n",
    "        if(in_len < 4):\n",
    "            in_layer = tf.expand_dims(in_layer, axis=expanded_axis)\n",
    "        out_layer = cbam_block(in_layer, ratio=ratio, residual = residual)\n",
    "    else:\n",
    "        raise Exception(\"'{}' is not supported attention module!\".format(attention_model))\n",
    "        \n",
    "    if (in_len == 3 and len(out_layer.shape) == 4):\n",
    "        out_layer = tf.squeeze(out_layer, expanded_axis)\n",
    "    elif (in_len == 4 and len(out_layer.shape) == 3):\n",
    "        out_layer = Reshape((in_sh[1], in_sh[2], in_sh[3]))(out_layer)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. TCN_block (Models.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TCN_block_(input_layer,input_dimension,depth,kernel_size,filters, dropout,\n",
    "               weightDecay = 0.009, maxNorm = 0.6, activation='relu'):\n",
    "    \"\"\" TCN_block from Bai et al 2018\n",
    "        Temporal Convolutional Network (TCN)\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        using different regularization methods\n",
    "    \"\"\"    \n",
    "    \n",
    "    block = Conv1D(filters, kernel_size=kernel_size, dilation_rate=1, activation='linear',\n",
    "                    kernel_regularizer=L2(weightDecay),\n",
    "                    kernel_constraint = max_norm(maxNorm, axis=[0,1]),\n",
    "                    \n",
    "                    padding = 'causal',kernel_initializer='he_uniform')(input_layer)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation(activation)(block)\n",
    "    block = Dropout(dropout)(block)\n",
    "    block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=1,activation='linear',\n",
    "                    kernel_regularizer=L2(weightDecay),\n",
    "                    kernel_constraint = max_norm(maxNorm, axis=[0,1]),\n",
    "\n",
    "                    padding = 'causal',kernel_initializer='he_uniform')(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation(activation)(block)\n",
    "    block = Dropout(dropout)(block)\n",
    "    if(input_dimension != filters):\n",
    "        conv = Conv1D(filters,kernel_size=1,\n",
    "                    kernel_regularizer=L2(weightDecay),\n",
    "                    kernel_constraint = max_norm(maxNorm, axis=[0,1]),\n",
    "                      \n",
    "                    padding='same')(input_layer)\n",
    "        added = Add()([block,conv])\n",
    "    else:\n",
    "        added = Add()([block,input_layer])\n",
    "    out = Activation(activation)(added)\n",
    "    \n",
    "    for i in range(depth-1):\n",
    "        block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=2**(i+1),activation='linear',\n",
    "                    kernel_regularizer=L2(weightDecay),\n",
    "                    kernel_constraint = max_norm(maxNorm, axis=[0,1]),\n",
    "                    \n",
    "                   padding = 'causal',kernel_initializer='he_uniform')(out)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation(activation)(block)\n",
    "        block = Dropout(dropout)(block)\n",
    "        block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=2**(i+1),activation='linear',\n",
    "                    kernel_regularizer=L2(weightDecay),\n",
    "                    kernel_constraint = max_norm(maxNorm, axis=[0,1]),\n",
    "\n",
    "                    padding = 'causal',kernel_initializer='he_uniform')(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation(activation)(block)\n",
    "        block = Dropout(dropout)(block)\n",
    "        added = Add()([block, out])\n",
    "        out = Activation(activation)(added)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. ATCNet architecture that uses Conv_block, attention_block and TCN_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ATCNet_(n_classes, in_chans = 22, in_samples = 1125, n_windows = 5, attention = 'mha', \n",
    "           eegn_F1 = 16, eegn_D = 2, eegn_kernelSize = 64, eegn_poolSize = 7, eegn_dropout=0.3, \n",
    "           tcn_depth = 2, tcn_kernelSize = 4, tcn_filters = 32, tcn_dropout = 0.3, \n",
    "           tcn_activation = 'elu', fuse = 'average'):\n",
    "    \n",
    "    \"\"\" ATCNet model from Altaheri et al 2023.\n",
    "        See details at https://ieeexplore.ieee.org/abstract/document/9852687\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "        The initial values in this model are based on the values identified by\n",
    "        the authors\n",
    "        \n",
    "        References\n",
    "        ----------\n",
    "        .. H. Altaheri, G. Muhammad, and M. Alsulaiman. \"Physics-informed \n",
    "           attention temporal convolutional network for EEG-based motor imagery \n",
    "           classification.\" IEEE Transactions on Industrial Informatics, \n",
    "           vol. 19, no. 2, pp. 2249-2258, (2023) \n",
    "           https://doi.org/10.1109/TII.2022.3197419\n",
    "    \"\"\"\n",
    "    input_1 = Input(shape = (1,in_chans, in_samples))   #     TensorShape([None, 1, 22, 1125])\n",
    "    input_2 = Permute((3,2,1))(input_1) \n",
    "\n",
    "    dense_weightDecay = 0.5  \n",
    "    conv_weightDecay = 0.009\n",
    "    conv_maxNorm = 0.6\n",
    "    from_logits = False\n",
    "\n",
    "    numFilters = eegn_F1\n",
    "    F2 = numFilters*eegn_D\n",
    "\n",
    "    block1 = Conv_block_(input_layer = input_2, F1 = eegn_F1, D = eegn_D, \n",
    "                        kernLength = eegn_kernelSize, poolSize = eegn_poolSize,\n",
    "                        weightDecay = conv_weightDecay, maxNorm = conv_maxNorm,\n",
    "                        in_chans = in_chans, dropout = eegn_dropout)\n",
    "    block1 = Lambda(lambda x: x[:,:,-1,:])(block1)\n",
    "       \n",
    "    # Sliding window \n",
    "    sw_concat = []   # to store concatenated or averaged sliding window outputs\n",
    "    for i in range(n_windows):\n",
    "        st = i\n",
    "        end = block1.shape[1]-n_windows+i+1\n",
    "        block2 = block1[:, st:end, :]\n",
    "        \n",
    "        # Attention_model\n",
    "        if attention is not None:\n",
    "            if (attention == 'se' or attention == 'cbam'):\n",
    "                block2 = Permute((2, 1))(block2) # shape=(None, 32, 16)\n",
    "                block2 = attention_block(block2, attention)\n",
    "                block2 = Permute((2, 1))(block2) # shape=(None, 16, 32)\n",
    "            else: block2 = attention_block(block2, attention)\n",
    "\n",
    "        # Temporal convolutional network (TCN)\n",
    "        block3 = TCN_block_(input_layer = block2, input_dimension = F2, depth = tcn_depth,\n",
    "                            kernel_size = tcn_kernelSize, filters = tcn_filters, \n",
    "                            weightDecay = conv_weightDecay, maxNorm = conv_maxNorm,\n",
    "                            dropout = tcn_dropout, activation = tcn_activation)\n",
    "        # Get feature maps of the last sequence\n",
    "        block3 = Lambda(lambda x: x[:,-1,:])(block3)\n",
    "        \n",
    "        # Outputs of sliding window: Average_after_dense or concatenate_then_dense\n",
    "        if(fuse == 'average'):\n",
    "            sw_concat.append(Dense(n_classes, kernel_regularizer=L2(dense_weightDecay))(block3))\n",
    "        elif(fuse == 'concat'):\n",
    "            if i == 0:\n",
    "                sw_concat = block3\n",
    "            else:\n",
    "                sw_concat = Concatenate()([sw_concat, block3])\n",
    "                \n",
    "    if(fuse == 'average'):\n",
    "        if len(sw_concat) > 1: # more than one window\n",
    "            sw_concat = tf.keras.layers.Average()(sw_concat[:])\n",
    "        else: # one window (# windows = 1)\n",
    "            sw_concat = sw_concat[0]\n",
    "    elif(fuse == 'concat'):\n",
    "        sw_concat = Dense(n_classes, kernel_regularizer=L2(dense_weightDecay))(sw_concat)\n",
    "               \n",
    "    if from_logits:  # No activation here because we are using from_logits=True\n",
    "        out = Activation('linear', name = 'linear')(sw_concat)\n",
    "    else:   # Using softmax activation\n",
    "        out = Activation('softmax', name = 'softmax')(sw_concat)\n",
    "       \n",
    "    return Model(inputs = input_1, outputs = out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
