{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "from keras.layers import LayerNormalization, MultiHeadAttention, Add, Dropout, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadInvertedAttention(tf.keras.layers.MultiHeadAttention):\n",
    "    \"\"\"Multi-head inverted attention block for iTransformer architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, query, value, attention_mask=None, training=None):\n",
    "        # Attention mechanism\n",
    "        attention_output = super().call(query, value, attention_mask=attention_mask, training=training)\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "def imha_block(input_feature, key_dim=8, num_heads=2, dropout=0.5):\n",
    "    \"\"\"iTransformer Multi-Head Inverted Attention block.\"\"\"\n",
    "    # Embedding layer (no embedding is required since the input features are already embedded)\n",
    "    x = input_feature\n",
    "    \n",
    "    # Multi-head inverted attention layer\n",
    "    x = MultiHeadInvertedAttention(key_dim=key_dim, num_heads=num_heads)(x, x)\n",
    "    \n",
    "    # Layer normalization across timestamps\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # Feed-forward network (FFN)\n",
    "    x = tf.keras.layers.Dense(units=x.shape[-1] * 4, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(units=x.shape[-1])(x)\n",
    "    \n",
    "    # Layer normalization across timestamps\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # Skip connection\n",
    "    imha_feature = Add()([input_feature, x])\n",
    "    \n",
    "    return imha_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_LSA(tf.keras.layers.MultiHeadAttention):\n",
    "    \"\"\"local multi-head self attention block\n",
    "     \n",
    "     Locality Self Attention as described in https://arxiv.org/abs/2112.13492v1\n",
    "     This implementation is taken from  https://keras.io/examples/vision/vit_small_ds/ \n",
    "    \"\"\"    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # The trainable temperature term. The initial value is the square \n",
    "        # root of the key dimension.\n",
    "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n",
    "\n",
    "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
    "        query = tf.multiply(query, 1.0 / self.tau)\n",
    "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "        attention_scores_dropout = self._dropout_layer(\n",
    "            attention_scores, training=training\n",
    "        )\n",
    "        attention_output = tf.einsum(\n",
    "            self._combine_equation, attention_scores_dropout, value\n",
    "        )\n",
    "        return attention_output, attention_scores\n",
    "\n",
    "\n",
    "\n",
    "def mha_block(input_feature, key_dim=8, num_heads=2, dropout = 0.5, vanilla = True):\n",
    "    \"\"\"Multi Head self Attention (MHA) block.     \n",
    "       \n",
    "    Here we include two types of MHA blocks: \n",
    "            The original multi-head self-attention as described in https://arxiv.org/abs/1706.03762\n",
    "            The multi-head local self attention as described in https://arxiv.org/abs/2112.13492v1\n",
    "    \"\"\"    \n",
    "    # Layer normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(input_feature)\n",
    "    \n",
    "    if vanilla:\n",
    "        # Create a multi-head attention layer as described in \n",
    "        # 'Attention Is All You Need' https://arxiv.org/abs/1706.03762\n",
    "        x = MultiHeadAttention(key_dim = key_dim, num_heads = num_heads, dropout = dropout)(x, x)\n",
    "    else:\n",
    "        # Create a multi-head local self-attention layer as described in \n",
    "        # 'Vision Transformer for Small-Size Datasets' https://arxiv.org/abs/2112.13492v1\n",
    "        \n",
    "        # Build the diagonal attention mask\n",
    "        NUM_PATCHES = input_feature.shape[1]\n",
    "        diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
    "        diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)\n",
    "        \n",
    "        # Create a multi-head local self attention layer.\n",
    "        # x = MultiHeadAttention_LSA(key_dim = key_dim, num_heads = num_heads, dropout = dropout)(\n",
    "        #     x, x, attention_mask = diag_attn_mask)\n",
    "        x = MultiHeadAttention_LSA(key_dim = key_dim, num_heads = num_heads, dropout = dropout)(\n",
    "            x, x, attention_mask = diag_attn_mask)\n",
    "    x = Dropout(0.3)(x)\n",
    "    # Skip connection\n",
    "    mha_feature = Add()([input_feature, x])\n",
    "    \n",
    "    return mha_feature\n",
    "\n",
    "\n",
    "def attention_block(in_layer, attention_model, ratio=8, residual = False, apply_to_input=True): \n",
    "    in_sh = in_layer.shape # dimensions of the input tensor\n",
    "    in_len = len(in_sh) \n",
    "    expanded_axis = 2 # defualt = 2\n",
    "    \n",
    "    if attention_model == 'mha':   # Multi-head self attention layer \n",
    "        if(in_len > 3):\n",
    "            in_layer = Reshape((in_sh[1],-1))(in_layer)\n",
    "        out_layer = mha_block(in_layer)\n",
    "    else:\n",
    "        raise Exception(\"'{}' is not supported attention module!\".format(attention_model))\n",
    "\n",
    "        \n",
    "    if (in_len == 3 and len(out_layer.shape) == 4):\n",
    "        out_layer = tf.squeeze(out_layer, expanded_axis)\n",
    "    elif (in_len == 4 and len(out_layer.shape) == 3):\n",
    "        out_layer = Reshape((in_sh[1], in_sh[2], in_sh[3]))(out_layer)\n",
    "    return out_layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
