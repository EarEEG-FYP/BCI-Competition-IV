{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. data_provider/dataloader.py\n",
    "\n",
    "Trying to import the UAEloader dataset which was used for time series classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#from utils.timefeatures import time_features\n",
    "from typing import List\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "\n",
    "#from data_provider.m4 import M4Dataset, M4Meta\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import pathlib\n",
    "import sys\n",
    "from urllib import request\n",
    "\n",
    "\n",
    "#from data_provider.uea import subsample, interpolate_missing, Normalizer\n",
    "\n",
    "\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data_provider/dataloader.py ------> utils/timefeatures.py\n",
    "\n",
    "Import time_features function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "\n",
    "\n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. data_provider/dataloader.py ------> dataprovider/m4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_file_name(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract file name from url.\n",
    "\n",
    "    :param url: URL to extract file name from.\n",
    "    :return: File name.\n",
    "    \"\"\"\n",
    "    return url.split('/')[-1] if len(url) > 0 else ''\n",
    "\n",
    "\n",
    "def download(url: str, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Download a file to the given path.\n",
    "\n",
    "    :param url: URL to download\n",
    "    :param file_path: Where to download the content.\n",
    "    \"\"\"\n",
    "\n",
    "    def progress(count, block_size, total_size):\n",
    "        progress_pct = float(count * block_size) / float(total_size) * 100.0\n",
    "        sys.stdout.write('\\rDownloading {} to {} {:.1f}%'.format(url, file_path, progress_pct))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        opener = request.build_opener()\n",
    "        opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "        request.install_opener(opener)\n",
    "        pathlib.Path(os.path.dirname(file_path)).mkdir(parents=True, exist_ok=True)\n",
    "        f, _ = request.urlretrieve(url, file_path, progress)\n",
    "        sys.stdout.write('\\n')\n",
    "        sys.stdout.flush()\n",
    "        file_info = os.stat(f)\n",
    "        logging.info(f'Successfully downloaded {os.path.basename(file_path)} {file_info.st_size} bytes.')\n",
    "    else:\n",
    "        file_info = os.stat(file_path)\n",
    "        logging.info(f'File already exists: {file_path} {file_info.st_size} bytes.')\n",
    "\n",
    "\n",
    "@dataclass()\n",
    "class M4Dataset:\n",
    "    ids: np.ndarray\n",
    "    groups: np.ndarray\n",
    "    frequencies: np.ndarray\n",
    "    horizons: np.ndarray\n",
    "    values: np.ndarray\n",
    "\n",
    "    @staticmethod\n",
    "    def load(training: bool = True, dataset_file: str = '../dataset/m4') -> 'M4Dataset':\n",
    "        \"\"\"\n",
    "        Load cached dataset.\n",
    "\n",
    "        :param training: Load training part if training is True, test part otherwise.\n",
    "        \"\"\"\n",
    "        info_file = os.path.join(dataset_file, 'M4-info.csv')\n",
    "        train_cache_file = os.path.join(dataset_file, 'training.npz')\n",
    "        test_cache_file = os.path.join(dataset_file, 'test.npz')\n",
    "        m4_info = pd.read_csv(info_file)\n",
    "        return M4Dataset(ids=m4_info.M4id.values,\n",
    "                         groups=m4_info.SP.values,\n",
    "                         frequencies=m4_info.Frequency.values,\n",
    "                         horizons=m4_info.Horizon.values,\n",
    "                         values=np.load(\n",
    "                             train_cache_file if training else test_cache_file,\n",
    "                             allow_pickle=True))\n",
    "\n",
    "\n",
    "@dataclass()\n",
    "class M4Meta:\n",
    "    seasonal_patterns = ['Yearly', 'Quarterly', 'Monthly', 'Weekly', 'Daily', 'Hourly']\n",
    "    horizons = [6, 8, 18, 13, 14, 48]\n",
    "    frequencies = [1, 4, 12, 1, 1, 24]\n",
    "    horizons_map = {\n",
    "        'Yearly': 6,\n",
    "        'Quarterly': 8,\n",
    "        'Monthly': 18,\n",
    "        'Weekly': 13,\n",
    "        'Daily': 14,\n",
    "        'Hourly': 48\n",
    "    }  # different predict length\n",
    "    frequency_map = {\n",
    "        'Yearly': 1,\n",
    "        'Quarterly': 4,\n",
    "        'Monthly': 12,\n",
    "        'Weekly': 1,\n",
    "        'Daily': 1,\n",
    "        'Hourly': 24\n",
    "    }\n",
    "    history_size = {\n",
    "        'Yearly': 1.5,\n",
    "        'Quarterly': 1.5,\n",
    "        'Monthly': 1.5,\n",
    "        'Weekly': 10,\n",
    "        'Daily': 10,\n",
    "        'Hourly': 10\n",
    "    }  # from interpretable.gin\n",
    "\n",
    "\n",
    "def load_m4_info() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load M4Info file.\n",
    "\n",
    "    :return: Pandas DataFrame of M4Info.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(INFO_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. data_provider/dataloader.py ------> data_provider/uea.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data, max_len=None):\n",
    "    \"\"\"Build mini-batch tensors from a list of (X, mask) tuples. Mask input. Create\n",
    "    Args:\n",
    "        data: len(batch_size) list of tuples (X, y).\n",
    "            - X: torch tensor of shape (seq_length, feat_dim); variable seq_length.\n",
    "            - y: torch tensor of shape (num_labels,) : class indices or numerical targets\n",
    "                (for classification or regression, respectively). num_labels > 1 for multi-task models\n",
    "        max_len: global fixed sequence length. Used for architectures requiring fixed length input,\n",
    "            where the batch length cannot vary dynamically. Longer sequences are clipped, shorter are padded with 0s\n",
    "    Returns:\n",
    "        X: (batch_size, padded_length, feat_dim) torch tensor of masked features (input)\n",
    "        targets: (batch_size, padded_length, feat_dim) torch tensor of unmasked features (output)\n",
    "        target_masks: (batch_size, padded_length, feat_dim) boolean torch tensor\n",
    "            0 indicates masked values to be predicted, 1 indicates unaffected/\"active\" feature values\n",
    "        padding_masks: (batch_size, padded_length) boolean tensor, 1 means keep vector at this position, 0 means padding\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(data)\n",
    "    features, labels = zip(*data)\n",
    "\n",
    "    # Stack and pad features and masks (convert 2D to 3D tensors, i.e. add batch dimension)\n",
    "    lengths = [X.shape[0] for X in features]  # original sequence length for each time series\n",
    "    if max_len is None:\n",
    "        max_len = max(lengths)\n",
    "\n",
    "    X = torch.zeros(batch_size, max_len, features[0].shape[-1])  # (batch_size, padded_length, feat_dim)\n",
    "    for i in range(batch_size):\n",
    "        end = min(lengths[i], max_len)\n",
    "        X[i, :end, :] = features[i][:end, :]\n",
    "\n",
    "    targets = torch.stack(labels, dim=0)  # (batch_size, num_labels)\n",
    "\n",
    "    padding_masks = padding_mask(torch.tensor(lengths, dtype=torch.int16),\n",
    "                                 max_len=max_len)  # (batch_size, padded_length) boolean tensor, \"1\" means keep\n",
    "\n",
    "    return X, targets, padding_masks\n",
    "\n",
    "\n",
    "def padding_mask(lengths, max_len=None):\n",
    "    \"\"\"\n",
    "    Used to mask padded positions: creates a (batch_size, max_len) boolean mask from a tensor of sequence lengths,\n",
    "    where 1 means keep element at this position (time step)\n",
    "    \"\"\"\n",
    "    batch_size = lengths.numel()\n",
    "    max_len = max_len or lengths.max_val()  # trick works because of overloading of 'or' operator for non-boolean types\n",
    "    return (torch.arange(0, max_len, device=lengths.device)\n",
    "            .type_as(lengths)\n",
    "            .repeat(batch_size, 1)\n",
    "            .lt(lengths.unsqueeze(1)))\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "    \"\"\"\n",
    "    Normalizes dataframe across ALL contained rows (time steps). Different from per-sample normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm_type='standardization', mean=None, std=None, min_val=None, max_val=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            norm_type: choose from:\n",
    "                \"standardization\", \"minmax\": normalizes dataframe across ALL contained rows (time steps)\n",
    "                \"per_sample_std\", \"per_sample_minmax\": normalizes each sample separately (i.e. across only its own rows)\n",
    "            mean, std, min_val, max_val: optional (num_feat,) Series of pre-computed values\n",
    "        \"\"\"\n",
    "\n",
    "        self.norm_type = norm_type\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def normalize(self, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: input dataframe\n",
    "        Returns:\n",
    "            df: normalized dataframe\n",
    "        \"\"\"\n",
    "        if self.norm_type == \"standardization\":\n",
    "            if self.mean is None:\n",
    "                self.mean = df.mean()\n",
    "                self.std = df.std()\n",
    "            return (df - self.mean) / (self.std + np.finfo(float).eps)\n",
    "\n",
    "        elif self.norm_type == \"minmax\":\n",
    "            if self.max_val is None:\n",
    "                self.max_val = df.max()\n",
    "                self.min_val = df.min()\n",
    "            return (df - self.min_val) / (self.max_val - self.min_val + np.finfo(float).eps)\n",
    "\n",
    "        elif self.norm_type == \"per_sample_std\":\n",
    "            grouped = df.groupby(by=df.index)\n",
    "            return (df - grouped.transform('mean')) / grouped.transform('std')\n",
    "\n",
    "        elif self.norm_type == \"per_sample_minmax\":\n",
    "            grouped = df.groupby(by=df.index)\n",
    "            min_vals = grouped.transform('min')\n",
    "            return (df - min_vals) / (grouped.transform('max') - min_vals + np.finfo(float).eps)\n",
    "\n",
    "        else:\n",
    "            raise (NameError(f'Normalize method \"{self.norm_type}\" not implemented'))\n",
    "\n",
    "\n",
    "def interpolate_missing(y):\n",
    "    \"\"\"\n",
    "    Replaces NaN values in pd.Series `y` using linear interpolation\n",
    "    \"\"\"\n",
    "    if y.isna().any():\n",
    "        y = y.interpolate(method='linear', limit_direction='both')\n",
    "    return y\n",
    "\n",
    "\n",
    "def subsample(y, limit=256, factor=2):\n",
    "    \"\"\"\n",
    "    If a given Series is longer than `limit`, returns subsampled sequence by the specified integer factor\n",
    "    \"\"\"\n",
    "    if len(y) > limit:\n",
    "        return y[::factor].reset_index(drop=True)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./all_datasets/EthanolConcentration/EthanolConcentration_TRAIN.ts', './all_datasets/EthanolConcentration/EthanolConcentration_TEST.ts']\n"
     ]
    }
   ],
   "source": [
    "root_path = './all_datasets/EthanolConcentration/'\n",
    "data_paths = glob(os.path.join(root_path, '*'))\n",
    "print(data_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. data_provider/dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UEAloader(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for datasets included in:\n",
    "        Time Series Classification Archive (www.timeseriesclassification.com)\n",
    "    Argument:\n",
    "        limit_size: float in (0, 1) for debug\n",
    "    Attributes:\n",
    "        all_df: (num_samples * seq_len, num_columns) dataframe indexed by integer indices, with multiple rows corresponding to the same index (sample).\n",
    "            Each row is a time step; Each column contains either metadata (e.g. timestamp) or a feature.\n",
    "        feature_df: (num_samples * seq_len, feat_dim) dataframe; contains the subset of columns of `all_df` which correspond to selected features\n",
    "        feature_names: names of columns contained in `feature_df` (same as feature_df.columns)\n",
    "        all_IDs: (num_samples,) series of IDs contained in `all_df`/`feature_df` (same as all_df.index.unique() )\n",
    "        labels_df: (num_samples, num_labels) pd.DataFrame of label(s) for each sample\n",
    "        max_seq_len: maximum sequence (time series) length. If None, script argument `max_seq_len` will be used.\n",
    "            (Moreover, script argument overrides this attribute)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_path, file_list=None, limit_size=None, flag=None):\n",
    "        self.root_path = root_path\n",
    "        self.all_df, self.labels_df = self.load_all(root_path, file_list=file_list, flag=flag)\n",
    "        self.all_IDs = self.all_df.index.unique()  # all sample IDs (integer indices 0 ... num_samples-1)\n",
    "\n",
    "        if limit_size is not None:\n",
    "            if limit_size > 1:\n",
    "                limit_size = int(limit_size)\n",
    "            else:  # interpret as proportion if in (0, 1]\n",
    "                limit_size = int(limit_size * len(self.all_IDs))\n",
    "            self.all_IDs = self.all_IDs[:limit_size]\n",
    "            self.all_df = self.all_df.loc[self.all_IDs]\n",
    "\n",
    "        # use all features\n",
    "        self.feature_names = self.all_df.columns\n",
    "        self.feature_df = self.all_df\n",
    "\n",
    "        # pre_process\n",
    "        normalizer = Normalizer()\n",
    "        self.feature_df = normalizer.normalize(self.feature_df)\n",
    "        print(len(self.all_IDs))\n",
    "\n",
    "    def load_all(self, root_path, file_list=None, flag=None):\n",
    "        \"\"\"\n",
    "        Loads datasets from csv files contained in `root_path` into a dataframe, optionally choosing from `pattern`\n",
    "        Args:\n",
    "            root_path: directory containing all individual .csv files\n",
    "            file_list: optionally, provide a list of file paths within `root_path` to consider.\n",
    "                Otherwise, entire `root_path` contents will be used.\n",
    "        Returns:\n",
    "            all_df: a single (possibly concatenated) dataframe with all data corresponding to specified files\n",
    "            labels_df: dataframe containing label(s) for each sample\n",
    "        \"\"\"\n",
    "        # Select paths for training and evaluation\n",
    "        if file_list is None:\n",
    "            data_paths = glob(os.path.join(root_path, '*'))  # list of all paths\n",
    "            \n",
    "        else:\n",
    "            data_paths = [os.path.join(root_path, p) for p in file_list]\n",
    "        if len(data_paths) == 0:\n",
    "            raise Exception('No files found using: {}'.format(os.path.join(root_path, '*')))\n",
    "        if flag is not None:\n",
    "            data_paths = list(filter(lambda x: re.search(flag, x), data_paths))\n",
    "        input_paths = [p for p in data_paths if os.path.isfile(p) and p.endswith('.ts')]\n",
    "        if len(input_paths) == 0:\n",
    "            raise Exception(\"No .ts files found using pattern: '{}'\".format(pattern))\n",
    "\n",
    "        all_df, labels_df = self.load_single(input_paths[0])  # a single file contains dataset\n",
    "\n",
    "        return all_df, labels_df\n",
    "\n",
    "    def load_single(self, filepath):\n",
    "        df, labels = load_from_tsfile_to_dataframe(filepath, return_separate_X_and_y=True,\n",
    "                                                             replace_missing_vals_with='NaN')\n",
    "        labels = pd.Series(labels, dtype=\"category\")\n",
    "        self.class_names = labels.cat.categories\n",
    "        labels_df = pd.DataFrame(labels.cat.codes,\n",
    "                                 dtype=np.int8)  # int8-32 gives an error when using nn.CrossEntropyLoss\n",
    "\n",
    "        lengths = df.applymap(\n",
    "            lambda x: len(x)).values  # (num_samples, num_dimensions) array containing the length of each series\n",
    "\n",
    "        horiz_diffs = np.abs(lengths - np.expand_dims(lengths[:, 0], -1))\n",
    "\n",
    "        if np.sum(horiz_diffs) > 0:  # if any row (sample) has varying length across dimensions\n",
    "            df = df.applymap(subsample)\n",
    "\n",
    "        lengths = df.applymap(lambda x: len(x)).values\n",
    "        vert_diffs = np.abs(lengths - np.expand_dims(lengths[0, :], 0))\n",
    "        if np.sum(vert_diffs) > 0:  # if any column (dimension) has varying length across samples\n",
    "            self.max_seq_len = int(np.max(lengths[:, 0]))\n",
    "        else:\n",
    "            self.max_seq_len = lengths[0, 0]\n",
    "\n",
    "        # First create a (seq_len, feat_dim) dataframe for each sample, indexed by a single integer (\"ID\" of the sample)\n",
    "        # Then concatenate into a (num_samples * seq_len, feat_dim) dataframe, with multiple rows corresponding to the\n",
    "        # sample index (i.e. the same scheme as all datasets in this project)\n",
    "\n",
    "        df = pd.concat((pd.DataFrame({col: df.loc[row, col] for col in df.columns}).reset_index(drop=True).set_index(\n",
    "            pd.Series(lengths[row, 0] * [row])) for row in range(df.shape[0])), axis=0)\n",
    "\n",
    "        # Replace NaN values\n",
    "        grp = df.groupby(by=df.index)\n",
    "        df = grp.transform(interpolate_missing)\n",
    "\n",
    "        return df, labels_df\n",
    "\n",
    "    def instance_norm(self, case):\n",
    "        if self.root_path.count('EthanolConcentration') > 0:  # special process for numerical stability\n",
    "            mean = case.mean(0, keepdim=True)\n",
    "            case = case - mean\n",
    "            stdev = torch.sqrt(torch.var(case, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            case /= stdev\n",
    "            return case\n",
    "        else:\n",
    "            return case\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return self.instance_norm(torch.from_numpy(self.feature_df.loc[self.all_IDs[ind]].values)), \\\n",
    "               torch.from_numpy(self.labels_df.loc[self.all_IDs[ind]].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_IDs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. DataProvider/data_factory.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only UEAloader is needed for time series classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    # 'ETTh1': Dataset_ETT_hour,\n",
    "    # 'ETTh2': Dataset_ETT_hour,\n",
    "    # 'ETTm1': Dataset_ETT_minute,\n",
    "    # 'ETTm2': Dataset_ETT_minute,\n",
    "    # 'custom': Dataset_Custom,\n",
    "    # 'm4': Dataset_M4,\n",
    "    # 'PSM': PSMSegLoader,\n",
    "    # 'MSL': MSLSegLoader,\n",
    "    # 'SMAP': SMAPSegLoader,\n",
    "    # 'SMD': SMDSegLoader,\n",
    "    # 'SWAT': SWATSegLoader,\n",
    "    'UEA': UEAloader\n",
    "}\n",
    "\n",
    "\n",
    "def data_provider(args, flag):\n",
    "    Data = data_dict[args.data]\n",
    "    timeenc = 0 if args.embed != 'timeF' else 1\n",
    "\n",
    "    if flag == 'test':\n",
    "        shuffle_flag = False\n",
    "        drop_last = True\n",
    "        if args.task_name == 'anomaly_detection' or args.task_name == 'classification':\n",
    "            batch_size = args.batch_size\n",
    "        else:\n",
    "            batch_size = 1  # bsz=1 for evaluation\n",
    "        freq = args.freq\n",
    "    else:\n",
    "        shuffle_flag = True\n",
    "        drop_last = True\n",
    "        batch_size = args.batch_size  # bsz for train and valid\n",
    "        freq = args.freq\n",
    "\n",
    "    if args.task_name == 'anomaly_detection':\n",
    "        drop_last = False\n",
    "        data_set = Data(\n",
    "            root_path=args.root_path,\n",
    "            win_size=args.seq_len,\n",
    "            flag=flag,\n",
    "        )\n",
    "        print(flag, len(data_set))\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=drop_last)\n",
    "        return data_set, data_loader\n",
    "    elif args.task_name == 'classification':\n",
    "        drop_last = False\n",
    "        data_set = Data(\n",
    "            root_path=args.root_path,\n",
    "            flag=flag,\n",
    "        )\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=drop_last,\n",
    "            collate_fn=lambda x: collate_fn(x, max_len=args.seq_len)\n",
    "        )\n",
    "        return data_set, data_loader\n",
    "    else:\n",
    "        if args.data == 'm4':\n",
    "            drop_last = False\n",
    "        data_set = Data(\n",
    "            root_path=args.root_path,\n",
    "            data_path=args.data_path,\n",
    "            flag=flag,\n",
    "            size=[args.seq_len, args.label_len, args.pred_len],\n",
    "            features=args.features,\n",
    "            target=args.target,\n",
    "            timeenc=timeenc,\n",
    "            freq=freq,\n",
    "            seasonal_patterns=args.seasonal_patterns\n",
    "        )\n",
    "        print(flag, len(data_set))\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=drop_last)\n",
    "        return data_set, data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from exp.exp_classification import Exp_Classification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from torch.optim import lr_scheduler\n",
    "import pdb\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from utils.str2bool import str2bool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from exp.exp_basic import Exp_Basic\n",
    "import os\n",
    "import torch\n",
    "\n",
    "#RevIN.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#ModernTCN_layer.py\n",
    "__all__ = ['moving_avg', 'series_decomp',  'Flatten_Head']\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "#ModernTCN\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from utils.tools import EarlyStopping, adjust_learning_rate, cal_accuracy\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "plt.switch_backend('agg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. run.py --------> tools/str2bool.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. run.py --------> exp/exp_classification.py --------> exp_basics.py ---------> models/ModernTCN.py ------> RevIN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode:str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else: raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim-1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:,-1,:].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps*self.eps)\n",
    "        x = x * self.stdev\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last\n",
    "        else:\n",
    "            x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. run.py --------> exp/exp_classification.py --------> exp_basics.py ---------> models/ModernTCN.py ------> models/ModernTCN_layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "# forecast task head\n",
    "class Flatten_Head(nn.Module):\n",
    "    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n",
    "        super(Flatten_Head, self).__init__()\n",
    "\n",
    "        self.individual = individual\n",
    "        self.n_vars = n_vars\n",
    "\n",
    "        if self.individual:\n",
    "            self.linears = nn.ModuleList()\n",
    "            self.dropouts = nn.ModuleList()\n",
    "            self.flattens = nn.ModuleList()\n",
    "            for i in range(self.n_vars):\n",
    "                self.flattens.append(nn.Flatten(start_dim=-2))\n",
    "                self.linears.append(nn.Linear(nf, target_window))\n",
    "                self.dropouts.append(nn.Dropout(head_dropout))\n",
    "        else:\n",
    "            self.flatten = nn.Flatten(start_dim=-2)\n",
    "            self.linear = nn.Linear(nf, target_window)\n",
    "            self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n",
    "        if self.individual:\n",
    "            x_out = []\n",
    "            for i in range(self.n_vars):\n",
    "                z = self.flattens[i](x[:, i, :, :])  # z: [bs x d_model * patch_num]\n",
    "                z = self.linears[i](z)  # z: [bs x target_window]\n",
    "                z = self.dropouts[i](z)\n",
    "                x_out.append(z)\n",
    "            x = torch.stack(x_out, dim=1)  # x: [bs x nvars x target_window]\n",
    "        else:\n",
    "            x = self.flatten(x)\n",
    "            x = self.linear(x)\n",
    "            x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. run.py --------> exp/exp_classification.py --------> exp_basics.py ---------> models/ModernTCN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.norm = nn.Layernorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, M, D, N = x.shape\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = x.reshape(B * M, N, D)\n",
    "        x = self.norm(\n",
    "            x)\n",
    "        x = x.reshape(B, M, N, D)\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        return x\n",
    "\n",
    "def get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias):\n",
    "    return nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "\n",
    "\n",
    "def get_bn(channels):\n",
    "    return nn.BatchNorm1d(channels)\n",
    "\n",
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups, dilation=1,bias=False):\n",
    "    if padding is None:\n",
    "        padding = kernel_size // 2\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv', get_conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                         stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias))\n",
    "    result.add_module('bn', get_bn(out_channels))\n",
    "    return result\n",
    "\n",
    "def fuse_bn(conv, bn):\n",
    "\n",
    "    kernel = conv.weight\n",
    "    running_mean = bn.running_mean\n",
    "    running_var = bn.running_var\n",
    "    gamma = bn.weight\n",
    "    beta = bn.bias\n",
    "    eps = bn.eps\n",
    "    std = (running_var + eps).sqrt()\n",
    "    t = (gamma / std).reshape(-1, 1, 1)\n",
    "    return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "class ReparamLargeKernelConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride, groups,\n",
    "                 small_kernel,\n",
    "                 small_kernel_merged=False, nvars=7):\n",
    "        super(ReparamLargeKernelConv, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.small_kernel = small_kernel\n",
    "        # We assume the conv does not change the feature map size, so padding = k//2. Otherwise, you may configure padding as you wish, and change the padding of small_conv accordingly.\n",
    "        padding = kernel_size // 2\n",
    "        if small_kernel_merged:\n",
    "            self.lkb_reparam = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                         stride=stride, padding=padding, dilation=1, groups=groups, bias=True)\n",
    "        else:\n",
    "            self.lkb_origin = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        stride=stride, padding=padding, dilation=1, groups=groups,bias=False)\n",
    "            if small_kernel is not None:\n",
    "                assert small_kernel <= kernel_size, 'The kernel size for re-param cannot be larger than the large kernel!'\n",
    "                self.small_conv = conv_bn(in_channels=in_channels, out_channels=out_channels,\n",
    "                                            kernel_size=small_kernel,\n",
    "                                            stride=stride, padding=small_kernel // 2, groups=groups, dilation=1,bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        if hasattr(self, 'lkb_reparam'):\n",
    "            out = self.lkb_reparam(inputs)\n",
    "        else:\n",
    "            out = self.lkb_origin(inputs)\n",
    "            if hasattr(self, 'small_conv'):\n",
    "                out += self.small_conv(inputs)\n",
    "        return out\n",
    "\n",
    "    def PaddingTwoEdge1d(self,x,pad_length_left,pad_length_right,pad_values=0):\n",
    "\n",
    "        D_out,D_in,ks=x.shape\n",
    "        if pad_values ==0:\n",
    "            pad_left = torch.zeros(D_out,D_in,pad_length_left)\n",
    "            pad_right = torch.zeros(D_out,D_in,pad_length_right)\n",
    "        else:\n",
    "            pad_left = torch.ones(D_out, D_in, pad_length_left) * pad_values\n",
    "            pad_right = torch.ones(D_out, D_in, pad_length_right) * pad_values\n",
    "        x = torch.cat([pad_left,x],dims=-1)\n",
    "        x = torch.cat([x,pad_right],dims=-1)\n",
    "        return x\n",
    "\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        eq_k, eq_b = fuse_bn(self.lkb_origin.conv, self.lkb_origin.bn)\n",
    "        if hasattr(self, 'small_conv'):\n",
    "            small_k, small_b = fuse_bn(self.small_conv.conv, self.small_conv.bn)\n",
    "            eq_b += small_b\n",
    "            eq_k += self.PaddingTwoEdge1d(small_k, (self.kernel_size - self.small_kernel) // 2,\n",
    "                                          (self.kernel_size - self.small_kernel) // 2, 0)\n",
    "        return eq_k, eq_b\n",
    "\n",
    "    def merge_kernel(self):\n",
    "        eq_k, eq_b = self.get_equivalent_kernel_bias()\n",
    "        self.lkb_reparam = nn.Conv1d(in_channels=self.lkb_origin.conv.in_channels,\n",
    "                                     out_channels=self.lkb_origin.conv.out_channels,\n",
    "                                     kernel_size=self.lkb_origin.conv.kernel_size, stride=self.lkb_origin.conv.stride,\n",
    "                                     padding=self.lkb_origin.conv.padding, dilation=self.lkb_origin.conv.dilation,\n",
    "                                     groups=self.lkb_origin.conv.groups, bias=True)\n",
    "        self.lkb_reparam.weight.data = eq_k\n",
    "        self.lkb_reparam.bias.data = eq_b\n",
    "        self.__delattr__('lkb_origin')\n",
    "        if hasattr(self, 'small_conv'):\n",
    "            self.__delattr__('small_conv')\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, large_size, small_size, dmodel, dff, nvars, small_kernel_merged=False, drop=0.1):\n",
    "\n",
    "        super(Block, self).__init__()\n",
    "        self.dw = ReparamLargeKernelConv(in_channels=nvars * dmodel, out_channels=nvars * dmodel,\n",
    "                                         kernel_size=large_size, stride=1, groups=nvars * dmodel,\n",
    "                                         small_kernel=small_size, small_kernel_merged=small_kernel_merged, nvars=nvars)\n",
    "        self.norm = nn.BatchNorm1d(dmodel)\n",
    "\n",
    "        #convffn1\n",
    "        self.ffn1pw1 = nn.Conv1d(in_channels=nvars * dmodel, out_channels=nvars * dff, kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=1, groups=nvars)\n",
    "        self.ffn1act = nn.GELU()\n",
    "        self.ffn1pw2 = nn.Conv1d(in_channels=nvars * dff, out_channels=nvars * dmodel, kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=1, groups=nvars)\n",
    "        self.ffn1drop1 = nn.Dropout(drop)\n",
    "        self.ffn1drop2 = nn.Dropout(drop)\n",
    "\n",
    "        #convffn2\n",
    "        self.ffn2pw1 = nn.Conv1d(in_channels=nvars * dmodel, out_channels=nvars * dff, kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=1, groups=dmodel)\n",
    "        self.ffn2act = nn.GELU()\n",
    "        self.ffn2pw2 = nn.Conv1d(in_channels=nvars * dff, out_channels=nvars * dmodel, kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=1, groups=dmodel)\n",
    "        self.ffn2drop1 = nn.Dropout(drop)\n",
    "        self.ffn2drop2 = nn.Dropout(drop)\n",
    "\n",
    "        self.ffn_ratio = dff//dmodel\n",
    "    def forward(self,x):\n",
    "\n",
    "        input = x\n",
    "        B, M, D, N = x.shape\n",
    "        x = x.reshape(B,M*D,N)\n",
    "        x = self.dw(x)\n",
    "        x = x.reshape(B,M,D,N)\n",
    "        x = x.reshape(B*M,D,N)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, M, D, N)\n",
    "        x = x.reshape(B, M * D, N)\n",
    "\n",
    "        x = self.ffn1drop1(self.ffn1pw1(x))\n",
    "        x = self.ffn1act(x)\n",
    "        x = self.ffn1drop2(self.ffn1pw2(x))\n",
    "        x = x.reshape(B, M, D, N)\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(B, D * M, N)\n",
    "        x = self.ffn2drop1(self.ffn2pw1(x))\n",
    "        x = self.ffn2act(x)\n",
    "        x = self.ffn2drop2(self.ffn2pw2(x))\n",
    "        x = x.reshape(B, D, M, N)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "\n",
    "        x = input + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Stage(nn.Module):\n",
    "    def __init__(self, ffn_ratio, num_blocks, large_size, small_size, dmodel, dw_model, nvars,\n",
    "                 small_kernel_merged=False, drop=0.1):\n",
    "\n",
    "        super(Stage, self).__init__()\n",
    "        d_ffn = dmodel * ffn_ratio\n",
    "        blks = []\n",
    "        for i in range(num_blocks):\n",
    "            blk = Block(large_size=large_size, small_size=small_size, dmodel=dmodel, dff=d_ffn, nvars=nvars, small_kernel_merged=small_kernel_merged, drop=drop)\n",
    "            blks.append(blk)\n",
    "\n",
    "        self.blocks = nn.ModuleList(blks)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ModernTCN(nn.Module):\n",
    "    def __init__(self,task_name,patch_size,patch_stride, stem_ratio, downsample_ratio, ffn_ratio, num_blocks, large_size, small_size, dims, dw_dims,\n",
    "                 nvars, small_kernel_merged=False, backbone_dropout=0.1, head_dropout=0.1, use_multi_scale=True, revin=True, affine=True,\n",
    "                 subtract_last=False, freq=None, seq_len=512, c_in=7, individual=False, target_window=96, class_drop=0.,class_num = 10):\n",
    "\n",
    "        super(ModernTCN, self).__init__()\n",
    "\n",
    "        self.task_name = task_name\n",
    "        self.class_drop = class_drop\n",
    "        self.class_num = class_num\n",
    "\n",
    "\n",
    "        # RevIN\n",
    "        self.revin = revin\n",
    "        if self.revin: self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n",
    "\n",
    "        # stem layer & down sampling layers\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv1d(1, dims[0], kernel_size=patch_size, stride=patch_stride),\n",
    "            nn.BatchNorm1d(dims[0])\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "\n",
    "        self.num_stage = len(num_blocks)\n",
    "        if self.num_stage > 1:\n",
    "            for i in range(self.num_stage - 1):\n",
    "                downsample_layer = nn.Sequential(\n",
    "                    nn.BatchNorm1d(dims[i]),\n",
    "                    nn.Conv1d(dims[i], dims[i + 1], kernel_size=downsample_ratio, stride=downsample_ratio),\n",
    "                )\n",
    "                self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_stride = patch_stride\n",
    "        self.downsample_ratio = downsample_ratio\n",
    "\n",
    "        # backbone\n",
    "        self.num_stage = len(num_blocks)\n",
    "        self.stages = nn.ModuleList()\n",
    "        for stage_idx in range(self.num_stage):\n",
    "            layer = Stage(ffn_ratio, num_blocks[stage_idx], large_size[stage_idx], small_size[stage_idx], dmodel=dims[stage_idx],\n",
    "                          dw_model=dw_dims[stage_idx], nvars=nvars, small_kernel_merged=small_kernel_merged, drop=backbone_dropout)\n",
    "            self.stages.append(layer)\n",
    "\n",
    "\n",
    "        # head\n",
    "        patch_num = seq_len // patch_stride\n",
    "        self.n_vars = c_in\n",
    "        self.individual = individual\n",
    "        d_model = dims[self.num_stage-1]\n",
    "\n",
    "\n",
    "        if use_multi_scale:\n",
    "            self.head_nf = d_model * patch_num\n",
    "            self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window,\n",
    "                                     head_dropout=head_dropout)\n",
    "        else:\n",
    "            if patch_num % pow(downsample_ratio,(self.num_stage - 1)) == 0:\n",
    "                self.head_nf = d_model * patch_num // pow(downsample_ratio,(self.num_stage - 1))\n",
    "            else:\n",
    "                self.head_nf = d_model * (patch_num // pow(downsample_ratio, (self.num_stage - 1))+1)\n",
    "\n",
    "\n",
    "            self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window,\n",
    "                                     head_dropout=head_dropout)\n",
    "\n",
    "        if self.task_name == 'classification':\n",
    "            self.act_class = F.gelu\n",
    "            self.class_dropout = nn.Dropout(self.class_drop)\n",
    "\n",
    "            self.head_class = nn.Linear(self.n_vars[0]*self.head_nf,self.class_num)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward_feature(self, x, te=None):\n",
    "\n",
    "        B,M,L=x.shape\n",
    "\n",
    "        x = x.unsqueeze(-2)\n",
    "\n",
    "        for i in range(self.num_stage):\n",
    "            B, M, D, N = x.shape\n",
    "            x = x.reshape(B * M, D, N)\n",
    "            if i==0:\n",
    "                if self.patch_size != self.patch_stride:\n",
    "                    # stem layer padding\n",
    "                    pad_len = self.patch_size - self.patch_stride\n",
    "                    pad = x[:,:,-1:].repeat(1,1,pad_len)\n",
    "                    x = torch.cat([x,pad],dim=-1)\n",
    "            else:\n",
    "                if N % self.downsample_ratio != 0:\n",
    "                    pad_len = self.downsample_ratio - (N % self.downsample_ratio)\n",
    "                    x = torch.cat([x, x[:, :, -pad_len:]],dim=-1)\n",
    "            x = self.downsample_layers[i](x)\n",
    "            _, D_, N_ = x.shape\n",
    "            x = x.reshape(B, M, D_, N_)\n",
    "            x = self.stages[i](x)\n",
    "        return x\n",
    "\n",
    "    def classification(self,x):\n",
    "\n",
    "        x =  self.forward_feature(x,te=None)\n",
    "        x = self.act_class(x)\n",
    "        x = self.class_dropout(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.head_class(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x, te=None):\n",
    "\n",
    "        if self.task_name == 'classification':\n",
    "            x = self.classification(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def structural_reparam(self):\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'merge_kernel'):\n",
    "                m.merge_kernel()\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        # hyper param\n",
    "        self.task_name = configs.task_name\n",
    "        self.stem_ratio = configs.stem_ratio\n",
    "        self.downsample_ratio = configs.downsample_ratio\n",
    "        self.ffn_ratio = configs.ffn_ratio\n",
    "        self.num_blocks = configs.num_blocks\n",
    "        self.large_size = configs.large_size\n",
    "        self.small_size = configs.small_size\n",
    "        self.dims = configs.dims\n",
    "        self.dw_dims = configs.dw_dims\n",
    "\n",
    "        self.nvars = configs.enc_in\n",
    "        self.small_kernel_merged = configs.small_kernel_merged\n",
    "        self.drop_backbone = configs.dropout\n",
    "        self.drop_head = configs.head_dropout\n",
    "        self.use_multi_scale = configs.use_multi_scale\n",
    "        self.revin = configs.revin\n",
    "        self.affine = configs.affine\n",
    "        self.subtract_last = configs.subtract_last\n",
    "\n",
    "        self.freq = configs.freq\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.c_in = self.nvars,\n",
    "        self.individual = configs.individual\n",
    "        self.target_window = configs.pred_len\n",
    "\n",
    "        self.kernel_size = configs.kernel_size\n",
    "        self.patch_size = configs.patch_size\n",
    "        self.patch_stride = configs.patch_stride\n",
    "\n",
    "        #classification\n",
    "        self.class_dropout = configs.class_dropout\n",
    "        self.class_num = configs.num_class\n",
    "\n",
    "\n",
    "        # decomp\n",
    "        self.decomposition = configs.decomposition\n",
    "\n",
    "\n",
    "        self.model = ModernTCN(task_name=self.task_name,patch_size=self.patch_size, patch_stride=self.patch_stride, stem_ratio=self.stem_ratio,\n",
    "                           downsample_ratio=self.downsample_ratio, ffn_ratio=self.ffn_ratio, num_blocks=self.num_blocks,\n",
    "                           large_size=self.large_size, small_size=self.small_size, dims=self.dims, dw_dims=self.dw_dims,\n",
    "                           nvars=self.nvars, small_kernel_merged=self.small_kernel_merged,\n",
    "                           backbone_dropout=self.drop_backbone, head_dropout=self.drop_head,\n",
    "                           use_multi_scale=self.use_multi_scale, revin=self.revin, affine=self.affine,\n",
    "                           subtract_last=self.subtract_last, freq=self.freq, seq_len=self.seq_len, c_in=self.c_in,\n",
    "                           individual=self.individual, target_window=self.target_window,\n",
    "                            class_drop = self.class_dropout, class_num = self.class_num)\n",
    "\n",
    "    def forward(self, x, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        te = None\n",
    "        x = self.model(x, te)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. run.py --------> exp/exp_classification.py --------> exp_basics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_Basic(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.model_dict = {\n",
    "\n",
    "            'ModernTCN':ModernTCN,\n",
    "\n",
    "\n",
    "        }\n",
    "        self.device = self._acquire_device()\n",
    "        self.model = self._build_model().to(self.device)\n",
    "\n",
    "    def _build_model(self):\n",
    "        raise NotImplementedError\n",
    "        return None\n",
    "\n",
    "    def _acquire_device(self):\n",
    "        if self.args.use_gpu:\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(\n",
    "                self.args.gpu) if not self.args.use_multi_gpu else self.args.devices\n",
    "            device = torch.device('cuda:{}'.format(self.args.gpu))\n",
    "            print('Use GPU: cuda:{}'.format(self.args.gpu))\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print('Use CPU')\n",
    "        return device\n",
    "\n",
    "    def _get_data(self):\n",
    "        pass\n",
    "\n",
    "    def vali(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. run.py --------> exp/exp_classification.py ----------> utils/tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, scheduler, epoch, args, printout=True):\n",
    "    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n",
    "    if args.lradj == 'type1':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "    elif args.lradj == 'type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "    elif args.lradj == 'type3':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 3 else args.learning_rate * (0.9 ** ((epoch - 3) // 1))}\n",
    "    elif args.lradj == 'type4':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 20 else args.learning_rate * (0.5 ** ((epoch // 20) // 1))}\n",
    "    elif args.lradj == 'type5':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate * (0.5 ** ((epoch // 10) // 1))}\n",
    "    elif args.lradj == 'type6':\n",
    "        lr_adjust = {20: args.learning_rate * 0.5 , 40: args.learning_rate * 0.01, 60:args.learning_rate * 0.01,8:args.learning_rate * 0.01,100:args.learning_rate * 0.01 }\n",
    "    elif args.lradj == 'constant':\n",
    "        lr_adjust = {epoch: args.learning_rate}\n",
    "    elif args.lradj == '3':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '4':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 15 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '5':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 25 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '6':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 5 else args.learning_rate*0.1}  \n",
    "    elif args.lradj == 'TST':\n",
    "        lr_adjust = {epoch: scheduler.get_last_lr()[0]}\n",
    "    \n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        if printout: print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "class StandardScaler():\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n",
    "\n",
    "\n",
    "def visual(true, preds=None, name='./pic/test.pdf'):\n",
    "    \"\"\"\n",
    "    Results visualization\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(true, label='GroundTruth', linewidth=2)\n",
    "    if preds is not None:\n",
    "        plt.plot(preds, label='Prediction', linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.savefig(name, bbox_inches='tight')\n",
    "\n",
    "def test_params_flop(model,x_shape):\n",
    "    \"\"\"\n",
    "    If you want to thest former's flop, you need to give default value to inputs in model.forward(), the following code can only pass one argument to forward()\n",
    "    \"\"\"\n",
    "    model_params = 0\n",
    "    for parameter in model.parameters():\n",
    "        model_params += parameter.numel()\n",
    "        print('INFO: Trainable parameter count: {:.2f}M'.format(model_params / 1000000.0))\n",
    "    from ptflops import get_model_complexity_info    \n",
    "    with torch.cuda.device(0):\n",
    "        macs, params = get_model_complexity_info(model.cuda(), x_shape, as_strings=True, print_per_layer_stat=True)\n",
    "        # print('Flops:' + flops)\n",
    "        # print('Params:' + params)\n",
    "        print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "def adjustment(gt, pred):\n",
    "    anomaly_state = False\n",
    "    for i in range(len(gt)):\n",
    "        if gt[i] == 1 and pred[i] == 1 and not anomaly_state:\n",
    "            anomaly_state = True\n",
    "            for j in range(i, 0, -1):\n",
    "                if gt[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if pred[j] == 0:\n",
    "                        pred[j] = 1\n",
    "            for j in range(i, len(gt)):\n",
    "                if gt[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if pred[j] == 0:\n",
    "                        pred[j] = 1\n",
    "        elif gt[i] == 0:\n",
    "            anomaly_state = False\n",
    "        if anomaly_state:\n",
    "            pred[i] = 1\n",
    "    return gt, pred\n",
    "\n",
    "def cal_accuracy(y_pred, y_true):\n",
    "    return np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. run.py --------> exp/exp_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_Classification(Exp_Basic):\n",
    "    def __init__(self, args):\n",
    "        super(Exp_Classification, self).__init__(args)\n",
    "\n",
    "    def _build_model(self):\n",
    "        # model input depends on data\n",
    "        train_data, train_loader = self._get_data(flag='TRAIN')\n",
    "        test_data, test_loader = self._get_data(flag='TEST')\n",
    "        self.args.seq_len = max(train_data.max_seq_len, test_data.max_seq_len)\n",
    "        self.args.pred_len = 0\n",
    "        self.args.enc_in = train_data.feature_df.shape[1]\n",
    "        self.args.num_class = len(train_data.class_names)\n",
    "        # model init\n",
    "        model = self.model_dict[self.args.model].Model(self.args).float()\n",
    "      \n",
    "        if self.args.use_multi_gpu and self.args.use_gpu:\n",
    "            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n",
    "        return model\n",
    "\n",
    "    def _get_data(self, flag):\n",
    "        data_set, data_loader = data_provider(self.args, flag)\n",
    "        return data_set, data_loader\n",
    "\n",
    "    def _select_optimizer(self):\n",
    "        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        return model_optim\n",
    "\n",
    "    def _select_criterion(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        return criterion\n",
    "\n",
    "    def vali(self, vali_data, vali_loader, criterion):\n",
    "        total_loss = []\n",
    "        preds = []\n",
    "        trues = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, label, padding_mask) in enumerate(vali_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                padding_mask = padding_mask.float().to(self.device)\n",
    "                label = label.to(self.device)\n",
    "\n",
    "                outputs = self.model(batch_x, padding_mask, None, None)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                loss = criterion(pred, label.long().squeeze().cpu())\n",
    "                total_loss.append(loss)\n",
    "\n",
    "                preds.append(outputs.detach())\n",
    "                trues.append(label)\n",
    "\n",
    "        total_loss = np.average(total_loss)\n",
    "\n",
    "        preds = torch.cat(preds, 0)\n",
    "        trues = torch.cat(trues, 0)\n",
    "        probs = torch.nn.functional.softmax(preds)  # (total_samples, num_classes) est. prob. for each class and sample\n",
    "        predictions = torch.argmax(probs, dim=1).cpu().numpy()  # (total_samples,) int class index for each sample\n",
    "        trues = trues.flatten().cpu().numpy()\n",
    "        accuracy = cal_accuracy(predictions, trues)\n",
    "\n",
    "        self.model.train()\n",
    "        return total_loss, accuracy\n",
    "\n",
    "    def train(self, setting):\n",
    "        train_data, train_loader = self._get_data(flag='TRAIN')\n",
    "        vali_data, vali_loader = self._get_data(flag='TEST')\n",
    "        test_data, test_loader = self._get_data(flag='TEST')\n",
    "\n",
    "        path = os.path.join(self.args.checkpoints, setting)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        time_now = time.time()\n",
    "\n",
    "        train_steps = len(train_loader)\n",
    "        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\n",
    "\n",
    "        model_optim = self._select_optimizer()\n",
    "        criterion = self._select_criterion()\n",
    "\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                            steps_per_epoch=train_steps,\n",
    "                                            pct_start=self.args.pct_start,\n",
    "                                            epochs=self.args.train_epochs,\n",
    "                                            max_lr=self.args.learning_rate)\n",
    "\n",
    "        for epoch in range(self.args.train_epochs):\n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "\n",
    "            self.model.train()\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            for i, (batch_x, label, padding_mask) in enumerate(train_loader):\n",
    "                iter_count += 1\n",
    "                model_optim.zero_grad()\n",
    "\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                padding_mask = padding_mask.float().to(self.device)\n",
    "                label = label.to(self.device)\n",
    "\n",
    "                outputs = self.model(batch_x, padding_mask, None, None)\n",
    "                loss = criterion(outputs, label.long().squeeze(-1))\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n",
    "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=4.0)\n",
    "                model_optim.step()\n",
    "\n",
    "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(train_loss)\n",
    "            vali_loss, val_accuracy = self.vali(vali_data, vali_loader, criterion)\n",
    "            test_loss, test_accuracy = self.vali(test_data, test_loader, criterion)\n",
    "\n",
    "            print(\n",
    "                \"Epoch: {0}, Steps: {1} | Train Loss: {2:.3f} Vali Loss: {3:.3f} Vali Acc: {4:.3f} Test Loss: {5:.3f} Test Acc: {6:.3f}\"\n",
    "                .format(epoch + 1, train_steps, train_loss, vali_loss, val_accuracy, test_loss, test_accuracy))\n",
    "            early_stopping(-val_accuracy, self.model, path)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                adjust_learning_rate(model_optim,  scheduler, epoch + 1, self.args)\n",
    "\n",
    "        best_model_path = path + '/' + 'checkpoint.pth'\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def test(self, setting, test=0):\n",
    "        test_data, test_loader = self._get_data(flag='TEST')\n",
    "        if test:\n",
    "            print('loading model')\n",
    "            self.model.load_state_dict(torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint.pth')))\n",
    "\n",
    "        preds = []\n",
    "        trues = []\n",
    "        folder_path = './test_results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, label, padding_mask) in enumerate(test_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                padding_mask = padding_mask.float().to(self.device)\n",
    "                label = label.to(self.device)\n",
    "\n",
    "                outputs = self.model(batch_x, padding_mask, None, None)\n",
    "\n",
    "                preds.append(outputs.detach())\n",
    "                trues.append(label)\n",
    "\n",
    "        preds = torch.cat(preds, 0)\n",
    "        trues = torch.cat(trues, 0)\n",
    "        print('test shape:', preds.shape, trues.shape)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(preds)  # (total_samples, num_classes) est. prob. for each class and sample\n",
    "        predictions = torch.argmax(probs, dim=1).cpu().numpy()  # (total_samples,) int class index for each sample\n",
    "        trues = trues.flatten().cpu().numpy()\n",
    "        accuracy = cal_accuracy(predictions, trues)\n",
    "\n",
    "        # result save\n",
    "        folder_path = './results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        print('accuracy:{}'.format(accuracy))\n",
    "        f = open(\"result_classification.txt\", 'a')\n",
    "        f.write(setting + \"  \\n\")\n",
    "        f.write('accuracy:{}'.format(accuracy))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. run.py using a Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n",
      "1\n",
      "EthanolConcentration\n"
     ]
    }
   ],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        # Random Seed\n",
    "        self.random_seed = 2021\n",
    "\n",
    "        # Basic Config\n",
    "        self.is_training = 1\n",
    "        self.model_id = 'EthanolConcentration'\n",
    "        self.model = 'ModernTCN'\n",
    "\n",
    "        # Data Loader\n",
    "        self.data = 'UEA'\n",
    "        self.root_path = './all_datasets/EthanolConcentration/'\n",
    "        self.data_path = 'ETTh1.csv'\n",
    "        self.features = 'M'\n",
    "        self.target = 'OT'\n",
    "        self.freq = 'h'\n",
    "        self.checkpoints = './checkpoints/'\n",
    "        self.embed = 'timeF'\n",
    "\n",
    "        # Forecasting Task\n",
    "        self.seq_len = 96\n",
    "        self.label_len = 48\n",
    "        self.pred_len = 96\n",
    "\n",
    "        # ModernTCN\n",
    "        self.stem_ratio = 6\n",
    "        self.downsample_ratio = 2\n",
    "        self.ffn_ratio = 1\n",
    "        self.patch_size = 8\n",
    "        self.patch_stride = 4\n",
    "        self.num_blocks = [1, 1, 1, 1]\n",
    "        self.large_size = [31, 29, 27, 13]\n",
    "        self.small_size = [5, 5, 5, 5]\n",
    "        self.dims = [256, 256, 256, 256]\n",
    "        self.dw_dims = [256, 256, 256, 256]\n",
    "        self.small_kernel_merged = False\n",
    "        self.call_structural_reparam = False\n",
    "        self.use_multi_scale = True\n",
    "\n",
    "        # PatchTST\n",
    "        self.fc_dropout = 0.05\n",
    "        self.head_dropout = 0.0\n",
    "        self.patch_len = 16\n",
    "        self.stride = 8\n",
    "        self.padding_patch = 'end'\n",
    "        self.revin = 1\n",
    "        self.affine = 0\n",
    "        self.subtract_last = 0\n",
    "        self.decomposition = 0\n",
    "        self.kernel_size = 25\n",
    "        self.individual = 0\n",
    "\n",
    "        # Formers\n",
    "        self.embed_type = 0\n",
    "        self.enc_in = 7\n",
    "        self.dec_in = 7\n",
    "        self.c_out = 7\n",
    "        self.d_model = 512\n",
    "        self.n_heads = 8\n",
    "        self.e_layers = 2\n",
    "        self.d_layers = 1\n",
    "        self.d_ff = 2048\n",
    "        self.moving_avg = 25\n",
    "        self.factor = 1\n",
    "        self.distil = True\n",
    "        self.dropout = 0.05\n",
    "        self.activation = 'gelu'\n",
    "        self.output_attention = False\n",
    "        self.do_predict = False\n",
    "\n",
    "        # Optimization\n",
    "        self.num_workers = 0\n",
    "        self.itr = 2\n",
    "        self.train_epochs = 100\n",
    "        self.batch_size = 128\n",
    "        self.patience = 100\n",
    "        self.learning_rate = 0.0001\n",
    "        self.des = 'test'\n",
    "        self.loss = 'mse'\n",
    "        self.lradj = 'type1'\n",
    "        self.pct_start = 0.3\n",
    "        self.use_amp = False\n",
    "\n",
    "        # GPU\n",
    "        self.use_gpu = True\n",
    "        self.gpu = 0\n",
    "        self.use_multi_gpu = False\n",
    "        self.devices = '0,1,2,3'\n",
    "        self.test_flop = False\n",
    "\n",
    "        # Multi Task\n",
    "        self.task_name = 'classification'\n",
    "\n",
    "        # Imputation Task\n",
    "        self.mask_rate = 0.25\n",
    "\n",
    "        # Anomaly Detection Task\n",
    "        self.anomaly_ratio = 0.25\n",
    "\n",
    "        # Classification Task\n",
    "        self.class_dropout = 0.05\n",
    "\n",
    "# Instantiate the class to access the parsed arguments\n",
    "args = ModelConfig()\n",
    "\n",
    "# Accessing the arguments\n",
    "print(args.random_seed)\n",
    "print(args.is_training)\n",
    "print(args.model_id)\n",
    "# Access other arguments similarly...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "<__main__.ModelConfig object at 0x15406c580>\n",
      "Use CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n",
      "263\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'ModernTCN' has no attribute 'Model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[397], line 48\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mitr):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# setting record of experiments\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     setting \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ft\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_sl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_pl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dim\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_nb\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_lk\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_sk\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ffr\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ps\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_str\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_multi\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_merged\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     30\u001b[0m         args\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m     31\u001b[0m         args\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m         args\u001b[38;5;241m.\u001b[39mdes,\n\u001b[1;32m     46\u001b[0m         ii)\n\u001b[0;32m---> 48\u001b[0m     exp \u001b[38;5;241m=\u001b[39m \u001b[43mExp\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# set experiments\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[1;32m     50\u001b[0m     exp\u001b[38;5;241m.\u001b[39mtrain(setting)\n",
      "Cell \u001b[0;32mIn[395], line 3\u001b[0m, in \u001b[0;36mExp_Classification.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mExp_Classification\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[393], line 11\u001b[0m, in \u001b[0;36mExp_Basic.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModernTCN\u001b[39m\u001b[38;5;124m'\u001b[39m:ModernTCN,\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire_device()\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[395], line 14\u001b[0m, in \u001b[0;36mExp_Classification._build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_data\u001b[38;5;241m.\u001b[39mclass_names)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# model init\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_multi_gpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_gpu:\n\u001b[1;32m     17\u001b[0m     model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(model, device_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice_ids)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'ModernTCN' has no attribute 'Model'"
     ]
    }
   ],
   "source": [
    "# random seed\n",
    "fix_seed = args.random_seed\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.dvices = args.devices.replace(' ', '')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #Exp = Exp_Main\n",
    "    if args.task_name == 'classification':\n",
    "        Exp = Exp_Classification\n",
    "    if args.large_size[0] < 13:\n",
    "        args.small_kernel_merged = True\n",
    "\n",
    "    if args.is_training:\n",
    "        for ii in range(args.itr):\n",
    "            # setting record of experiments\n",
    "            setting = '{}_{}_{}_ft{}_sl{}_pl{}_dim{}_nb{}_lk{}_sk{}_ffr{}_ps{}_str{}_multi{}_merged{}_{}_{}'.format(\n",
    "                args.model_id,\n",
    "                args.model,\n",
    "                args.data,\n",
    "                args.features,\n",
    "                args.seq_len,\n",
    "                args.pred_len,\n",
    "                args.dims[0],\n",
    "                args.num_blocks[0],\n",
    "                args.large_size[0],\n",
    "                args.small_size[0],\n",
    "                args.ffn_ratio,\n",
    "                args.patch_size,\n",
    "                args.patch_stride,\n",
    "                args.use_multi_scale,\n",
    "                args.small_kernel_merged,\n",
    "                args.des,\n",
    "                ii)\n",
    "\n",
    "            exp = Exp(args)  # set experiments\n",
    "            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "            exp.train(setting)\n",
    "\n",
    "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.test(setting)\n",
    "\n",
    "            if args.do_predict:\n",
    "                print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "                exp.predict(setting, True)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        ii = 0\n",
    "        setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(args.model_id,\n",
    "                                                                                                      args.model,\n",
    "                                                                                                      args.data,\n",
    "                                                                                                      args.features,\n",
    "                                                                                                      args.seq_len,\n",
    "                                                                                                      args.label_len,\n",
    "                                                                                                      args.pred_len,\n",
    "                                                                                                      args.d_model,\n",
    "                                                                                                      args.n_heads,\n",
    "                                                                                                      args.e_layers,\n",
    "                                                                                                      args.d_layers,\n",
    "                                                                                                      args.d_ff,\n",
    "                                                                                                      args.factor,\n",
    "                                                                                                      args.embed,\n",
    "                                                                                                      args.distil,\n",
    "                                                                                                      args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting, test=1)\n",
    "        torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
