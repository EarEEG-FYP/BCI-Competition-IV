{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. data_provider/dataloader.py\n",
    "\n",
    "Trying to import the UAEloader dataset which was used for time series classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#from utils.timefeatures import time_features\n",
    "from typing import List\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "\n",
    "#from data_provider.m4 import M4Dataset, M4Meta\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import pathlib\n",
    "import sys\n",
    "from urllib import request\n",
    "\n",
    "\n",
    "#from data_provider.uea import subsample, interpolate_missing, Normalizer\n",
    "\n",
    "\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data_provider/dataloader.py ------> utils/timefeatures.py\n",
    "\n",
    "Import time_features function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "\n",
    "\n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. data_provider/dataloader.py ------> dataprovider/m4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_file_name(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract file name from url.\n",
    "\n",
    "    :param url: URL to extract file name from.\n",
    "    :return: File name.\n",
    "    \"\"\"\n",
    "    return url.split('/')[-1] if len(url) > 0 else ''\n",
    "\n",
    "\n",
    "def download(url: str, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Download a file to the given path.\n",
    "\n",
    "    :param url: URL to download\n",
    "    :param file_path: Where to download the content.\n",
    "    \"\"\"\n",
    "\n",
    "    def progress(count, block_size, total_size):\n",
    "        progress_pct = float(count * block_size) / float(total_size) * 100.0\n",
    "        sys.stdout.write('\\rDownloading {} to {} {:.1f}%'.format(url, file_path, progress_pct))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        opener = request.build_opener()\n",
    "        opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "        request.install_opener(opener)\n",
    "        pathlib.Path(os.path.dirname(file_path)).mkdir(parents=True, exist_ok=True)\n",
    "        f, _ = request.urlretrieve(url, file_path, progress)\n",
    "        sys.stdout.write('\\n')\n",
    "        sys.stdout.flush()\n",
    "        file_info = os.stat(f)\n",
    "        logging.info(f'Successfully downloaded {os.path.basename(file_path)} {file_info.st_size} bytes.')\n",
    "    else:\n",
    "        file_info = os.stat(file_path)\n",
    "        logging.info(f'File already exists: {file_path} {file_info.st_size} bytes.')\n",
    "\n",
    "\n",
    "@dataclass()\n",
    "class M4Dataset:\n",
    "    ids: np.ndarray\n",
    "    groups: np.ndarray\n",
    "    frequencies: np.ndarray\n",
    "    horizons: np.ndarray\n",
    "    values: np.ndarray\n",
    "\n",
    "    @staticmethod\n",
    "    def load(training: bool = True, dataset_file: str = '../dataset/m4') -> 'M4Dataset':\n",
    "        \"\"\"\n",
    "        Load cached dataset.\n",
    "\n",
    "        :param training: Load training part if training is True, test part otherwise.\n",
    "        \"\"\"\n",
    "        info_file = os.path.join(dataset_file, 'M4-info.csv')\n",
    "        train_cache_file = os.path.join(dataset_file, 'training.npz')\n",
    "        test_cache_file = os.path.join(dataset_file, 'test.npz')\n",
    "        m4_info = pd.read_csv(info_file)\n",
    "        return M4Dataset(ids=m4_info.M4id.values,\n",
    "                         groups=m4_info.SP.values,\n",
    "                         frequencies=m4_info.Frequency.values,\n",
    "                         horizons=m4_info.Horizon.values,\n",
    "                         values=np.load(\n",
    "                             train_cache_file if training else test_cache_file,\n",
    "                             allow_pickle=True))\n",
    "\n",
    "\n",
    "@dataclass()\n",
    "class M4Meta:\n",
    "    seasonal_patterns = ['Yearly', 'Quarterly', 'Monthly', 'Weekly', 'Daily', 'Hourly']\n",
    "    horizons = [6, 8, 18, 13, 14, 48]\n",
    "    frequencies = [1, 4, 12, 1, 1, 24]\n",
    "    horizons_map = {\n",
    "        'Yearly': 6,\n",
    "        'Quarterly': 8,\n",
    "        'Monthly': 18,\n",
    "        'Weekly': 13,\n",
    "        'Daily': 14,\n",
    "        'Hourly': 48\n",
    "    }  # different predict length\n",
    "    frequency_map = {\n",
    "        'Yearly': 1,\n",
    "        'Quarterly': 4,\n",
    "        'Monthly': 12,\n",
    "        'Weekly': 1,\n",
    "        'Daily': 1,\n",
    "        'Hourly': 24\n",
    "    }\n",
    "    history_size = {\n",
    "        'Yearly': 1.5,\n",
    "        'Quarterly': 1.5,\n",
    "        'Monthly': 1.5,\n",
    "        'Weekly': 10,\n",
    "        'Daily': 10,\n",
    "        'Hourly': 10\n",
    "    }  # from interpretable.gin\n",
    "\n",
    "\n",
    "def load_m4_info() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load M4Info file.\n",
    "\n",
    "    :return: Pandas DataFrame of M4Info.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(INFO_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. data_provider/dataloader.py ------> data_provider/uea.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data, max_len=None):\n",
    "    \"\"\"Build mini-batch tensors from a list of (X, mask) tuples. Mask input. Create\n",
    "    Args:\n",
    "        data: len(batch_size) list of tuples (X, y).\n",
    "            - X: torch tensor of shape (seq_length, feat_dim); variable seq_length.\n",
    "            - y: torch tensor of shape (num_labels,) : class indices or numerical targets\n",
    "                (for classification or regression, respectively). num_labels > 1 for multi-task models\n",
    "        max_len: global fixed sequence length. Used for architectures requiring fixed length input,\n",
    "            where the batch length cannot vary dynamically. Longer sequences are clipped, shorter are padded with 0s\n",
    "    Returns:\n",
    "        X: (batch_size, padded_length, feat_dim) torch tensor of masked features (input)\n",
    "        targets: (batch_size, padded_length, feat_dim) torch tensor of unmasked features (output)\n",
    "        target_masks: (batch_size, padded_length, feat_dim) boolean torch tensor\n",
    "            0 indicates masked values to be predicted, 1 indicates unaffected/\"active\" feature values\n",
    "        padding_masks: (batch_size, padded_length) boolean tensor, 1 means keep vector at this position, 0 means padding\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(data)\n",
    "    features, labels = zip(*data)\n",
    "\n",
    "    # Stack and pad features and masks (convert 2D to 3D tensors, i.e. add batch dimension)\n",
    "    lengths = [X.shape[0] for X in features]  # original sequence length for each time series\n",
    "    if max_len is None:\n",
    "        max_len = max(lengths)\n",
    "\n",
    "    X = torch.zeros(batch_size, max_len, features[0].shape[-1])  # (batch_size, padded_length, feat_dim)\n",
    "    for i in range(batch_size):\n",
    "        end = min(lengths[i], max_len)\n",
    "        X[i, :end, :] = features[i][:end, :]\n",
    "\n",
    "    targets = torch.stack(labels, dim=0)  # (batch_size, num_labels)\n",
    "\n",
    "    padding_masks = padding_mask(torch.tensor(lengths, dtype=torch.int16),\n",
    "                                 max_len=max_len)  # (batch_size, padded_length) boolean tensor, \"1\" means keep\n",
    "\n",
    "    return X, targets, padding_masks\n",
    "\n",
    "\n",
    "def padding_mask(lengths, max_len=None):\n",
    "    \"\"\"\n",
    "    Used to mask padded positions: creates a (batch_size, max_len) boolean mask from a tensor of sequence lengths,\n",
    "    where 1 means keep element at this position (time step)\n",
    "    \"\"\"\n",
    "    batch_size = lengths.numel()\n",
    "    max_len = max_len or lengths.max_val()  # trick works because of overloading of 'or' operator for non-boolean types\n",
    "    return (torch.arange(0, max_len, device=lengths.device)\n",
    "            .type_as(lengths)\n",
    "            .repeat(batch_size, 1)\n",
    "            .lt(lengths.unsqueeze(1)))\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "    \"\"\"\n",
    "    Normalizes dataframe across ALL contained rows (time steps). Different from per-sample normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm_type='standardization', mean=None, std=None, min_val=None, max_val=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            norm_type: choose from:\n",
    "                \"standardization\", \"minmax\": normalizes dataframe across ALL contained rows (time steps)\n",
    "                \"per_sample_std\", \"per_sample_minmax\": normalizes each sample separately (i.e. across only its own rows)\n",
    "            mean, std, min_val, max_val: optional (num_feat,) Series of pre-computed values\n",
    "        \"\"\"\n",
    "\n",
    "        self.norm_type = norm_type\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def normalize(self, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: input dataframe\n",
    "        Returns:\n",
    "            df: normalized dataframe\n",
    "        \"\"\"\n",
    "        if self.norm_type == \"standardization\":\n",
    "            if self.mean is None:\n",
    "                self.mean = df.mean()\n",
    "                self.std = df.std()\n",
    "            return (df - self.mean) / (self.std + np.finfo(float).eps)\n",
    "\n",
    "        elif self.norm_type == \"minmax\":\n",
    "            if self.max_val is None:\n",
    "                self.max_val = df.max()\n",
    "                self.min_val = df.min()\n",
    "            return (df - self.min_val) / (self.max_val - self.min_val + np.finfo(float).eps)\n",
    "\n",
    "        elif self.norm_type == \"per_sample_std\":\n",
    "            grouped = df.groupby(by=df.index)\n",
    "            return (df - grouped.transform('mean')) / grouped.transform('std')\n",
    "\n",
    "        elif self.norm_type == \"per_sample_minmax\":\n",
    "            grouped = df.groupby(by=df.index)\n",
    "            min_vals = grouped.transform('min')\n",
    "            return (df - min_vals) / (grouped.transform('max') - min_vals + np.finfo(float).eps)\n",
    "\n",
    "        else:\n",
    "            raise (NameError(f'Normalize method \"{self.norm_type}\" not implemented'))\n",
    "\n",
    "\n",
    "def interpolate_missing(y):\n",
    "    \"\"\"\n",
    "    Replaces NaN values in pd.Series `y` using linear interpolation\n",
    "    \"\"\"\n",
    "    if y.isna().any():\n",
    "        y = y.interpolate(method='linear', limit_direction='both')\n",
    "    return y\n",
    "\n",
    "\n",
    "def subsample(y, limit=256, factor=2):\n",
    "    \"\"\"\n",
    "    If a given Series is longer than `limit`, returns subsampled sequence by the specified integer factor\n",
    "    \"\"\"\n",
    "    if len(y) > limit:\n",
    "        return y[::factor].reset_index(drop=True)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. data_provider/dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UEAloader(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for datasets included in:\n",
    "        Time Series Classification Archive (www.timeseriesclassification.com)\n",
    "    Argument:\n",
    "        limit_size: float in (0, 1) for debug\n",
    "    Attributes:\n",
    "        all_df: (num_samples * seq_len, num_columns) dataframe indexed by integer indices, with multiple rows corresponding to the same index (sample).\n",
    "            Each row is a time step; Each column contains either metadata (e.g. timestamp) or a feature.\n",
    "        feature_df: (num_samples * seq_len, feat_dim) dataframe; contains the subset of columns of `all_df` which correspond to selected features\n",
    "        feature_names: names of columns contained in `feature_df` (same as feature_df.columns)\n",
    "        all_IDs: (num_samples,) series of IDs contained in `all_df`/`feature_df` (same as all_df.index.unique() )\n",
    "        labels_df: (num_samples, num_labels) pd.DataFrame of label(s) for each sample\n",
    "        max_seq_len: maximum sequence (time series) length. If None, script argument `max_seq_len` will be used.\n",
    "            (Moreover, script argument overrides this attribute)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_path, file_list=None, limit_size=None, flag=None):\n",
    "        self.root_path = root_path\n",
    "        self.all_df, self.labels_df = self.load_all(root_path, file_list=file_list, flag=flag)\n",
    "        self.all_IDs = self.all_df.index.unique()  # all sample IDs (integer indices 0 ... num_samples-1)\n",
    "\n",
    "        if limit_size is not None:\n",
    "            if limit_size > 1:\n",
    "                limit_size = int(limit_size)\n",
    "            else:  # interpret as proportion if in (0, 1]\n",
    "                limit_size = int(limit_size * len(self.all_IDs))\n",
    "            self.all_IDs = self.all_IDs[:limit_size]\n",
    "            self.all_df = self.all_df.loc[self.all_IDs]\n",
    "\n",
    "        # use all features\n",
    "        self.feature_names = self.all_df.columns\n",
    "        self.feature_df = self.all_df\n",
    "\n",
    "        # pre_process\n",
    "        normalizer = Normalizer()\n",
    "        self.feature_df = normalizer.normalize(self.feature_df)\n",
    "        print(len(self.all_IDs))\n",
    "\n",
    "    def load_all(self, root_path, file_list=None, flag=None):\n",
    "        \"\"\"\n",
    "        Loads datasets from csv files contained in `root_path` into a dataframe, optionally choosing from `pattern`\n",
    "        Args:\n",
    "            root_path: directory containing all individual .csv files\n",
    "            file_list: optionally, provide a list of file paths within `root_path` to consider.\n",
    "                Otherwise, entire `root_path` contents will be used.\n",
    "        Returns:\n",
    "            all_df: a single (possibly concatenated) dataframe with all data corresponding to specified files\n",
    "            labels_df: dataframe containing label(s) for each sample\n",
    "        \"\"\"\n",
    "        # Select paths for training and evaluation\n",
    "        if file_list is None:\n",
    "            data_paths = glob.glob(os.path.join(root_path, '*'))  # list of all paths\n",
    "        else:\n",
    "            data_paths = [os.path.join(root_path, p) for p in file_list]\n",
    "        if len(data_paths) == 0:\n",
    "            raise Exception('No files found using: {}'.format(os.path.join(root_path, '*')))\n",
    "        if flag is not None:\n",
    "            data_paths = list(filter(lambda x: re.search(flag, x), data_paths))\n",
    "        input_paths = [p for p in data_paths if os.path.isfile(p) and p.endswith('.ts')]\n",
    "        if len(input_paths) == 0:\n",
    "            raise Exception(\"No .ts files found using pattern: '{}'\".format(pattern))\n",
    "\n",
    "        all_df, labels_df = self.load_single(input_paths[0])  # a single file contains dataset\n",
    "\n",
    "        return all_df, labels_df\n",
    "\n",
    "    def load_single(self, filepath):\n",
    "        df, labels = load_from_tsfile_to_dataframe(filepath, return_separate_X_and_y=True,\n",
    "                                                             replace_missing_vals_with='NaN')\n",
    "        labels = pd.Series(labels, dtype=\"category\")\n",
    "        self.class_names = labels.cat.categories\n",
    "        labels_df = pd.DataFrame(labels.cat.codes,\n",
    "                                 dtype=np.int8)  # int8-32 gives an error when using nn.CrossEntropyLoss\n",
    "\n",
    "        lengths = df.applymap(\n",
    "            lambda x: len(x)).values  # (num_samples, num_dimensions) array containing the length of each series\n",
    "\n",
    "        horiz_diffs = np.abs(lengths - np.expand_dims(lengths[:, 0], -1))\n",
    "\n",
    "        if np.sum(horiz_diffs) > 0:  # if any row (sample) has varying length across dimensions\n",
    "            df = df.applymap(subsample)\n",
    "\n",
    "        lengths = df.applymap(lambda x: len(x)).values\n",
    "        vert_diffs = np.abs(lengths - np.expand_dims(lengths[0, :], 0))\n",
    "        if np.sum(vert_diffs) > 0:  # if any column (dimension) has varying length across samples\n",
    "            self.max_seq_len = int(np.max(lengths[:, 0]))\n",
    "        else:\n",
    "            self.max_seq_len = lengths[0, 0]\n",
    "\n",
    "        # First create a (seq_len, feat_dim) dataframe for each sample, indexed by a single integer (\"ID\" of the sample)\n",
    "        # Then concatenate into a (num_samples * seq_len, feat_dim) dataframe, with multiple rows corresponding to the\n",
    "        # sample index (i.e. the same scheme as all datasets in this project)\n",
    "\n",
    "        df = pd.concat((pd.DataFrame({col: df.loc[row, col] for col in df.columns}).reset_index(drop=True).set_index(\n",
    "            pd.Series(lengths[row, 0] * [row])) for row in range(df.shape[0])), axis=0)\n",
    "\n",
    "        # Replace NaN values\n",
    "        grp = df.groupby(by=df.index)\n",
    "        df = grp.transform(interpolate_missing)\n",
    "\n",
    "        return df, labels_df\n",
    "\n",
    "    def instance_norm(self, case):\n",
    "        if self.root_path.count('EthanolConcentration') > 0:  # special process for numerical stability\n",
    "            mean = case.mean(0, keepdim=True)\n",
    "            case = case - mean\n",
    "            stdev = torch.sqrt(torch.var(case, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            case /= stdev\n",
    "            return case\n",
    "        else:\n",
    "            return case\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return self.instance_norm(torch.from_numpy(self.feature_df.loc[self.all_IDs[ind]].values)), \\\n",
    "               torch.from_numpy(self.labels_df.loc[self.all_IDs[ind]].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_IDs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. DataProvider/data_factory.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only UEAloader is needed for time series classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    # 'ETTh1': Dataset_ETT_hour,\n",
    "    # 'ETTh2': Dataset_ETT_hour,\n",
    "    # 'ETTm1': Dataset_ETT_minute,\n",
    "    # 'ETTm2': Dataset_ETT_minute,\n",
    "    # 'custom': Dataset_Custom,\n",
    "    # 'm4': Dataset_M4,\n",
    "    # 'PSM': PSMSegLoader,\n",
    "    # 'MSL': MSLSegLoader,\n",
    "    # 'SMAP': SMAPSegLoader,\n",
    "    # 'SMD': SMDSegLoader,\n",
    "    # 'SWAT': SWATSegLoader,\n",
    "    'UEA': UEAloader\n",
    "}\n",
    "\n",
    "\n",
    "def data_provider(args, flag):\n",
    "    Data = data_dict[args.data]\n",
    "    timeenc = 0 if args.embed != 'timeF' else 1\n",
    "\n",
    "    if flag == 'test':\n",
    "        shuffle_flag = False\n",
    "        drop_last = True\n",
    "        if args.task_name == 'anomaly_detection' or args.task_name == 'classification':\n",
    "            batch_size = args.batch_size\n",
    "        else:\n",
    "            batch_size = 1  # bsz=1 for evaluation\n",
    "        freq = args.freq\n",
    "    else:\n",
    "        shuffle_flag = True\n",
    "        drop_last = True\n",
    "        batch_size = args.batch_size  # bsz for train and valid\n",
    "        freq = args.freq\n",
    "\n",
    "    if args.task_name == 'anomaly_detection':\n",
    "        drop_last = False\n",
    "        data_set = Data(\n",
    "            root_path=args.root_path,\n",
    "            win_size=args.seq_len,\n",
    "            flag=flag,\n",
    "        )\n",
    "        print(flag, len(data_set))\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=drop_last)\n",
    "        return data_set, data_loader\n",
    "    elif args.task_name == 'classification':\n",
    "        drop_last = False\n",
    "        data_set = Data(\n",
    "            root_path=args.root_path,\n",
    "            flag=flag,\n",
    "        )\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=drop_last,\n",
    "            collate_fn=lambda x: collate_fn(x, max_len=args.seq_len)\n",
    "        )\n",
    "        return data_set, data_loader\n",
    "    else:\n",
    "        if args.data == 'm4':\n",
    "            drop_last = False\n",
    "        data_set = Data(\n",
    "            root_path=args.root_path,\n",
    "            data_path=args.data_path,\n",
    "            flag=flag,\n",
    "            size=[args.seq_len, args.label_len, args.pred_len],\n",
    "            features=args.features,\n",
    "            target=args.target,\n",
    "            timeenc=timeenc,\n",
    "            freq=freq,\n",
    "            seasonal_patterns=args.seasonal_patterns\n",
    "        )\n",
    "        print(flag, len(data_set))\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=drop_last)\n",
    "        return data_set, data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from exp.exp_classification import Exp_Classification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from torch.optim import lr_scheduler\n",
    "import pdb\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from utils.str2bool import str2bool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from exp.exp_basic import Exp_Basic\n",
    "import os\n",
    "import torch\n",
    "\n",
    "#RevIN.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#ModernTCN_layer.py\n",
    "__all__ = ['moving_avg', 'series_decomp',  'Flatten_Head']\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "#ModernTCN\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from utils.tools import EarlyStopping, adjust_learning_rate, cal_accuracy\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "plt.switch_backend('agg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. run.py --------> tools/str2bool.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. run.py --------> exp/exp_classification.py --------> exp_basics.py ---------> models/ModernTCN.py ------> RevIN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode:str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else: raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim-1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:,-1,:].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps*self.eps)\n",
    "        x = x * self.stdev\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last\n",
    "        else:\n",
    "            x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. run.py --------> exp/exp_classification.py --------> exp_basics.py ---------> models/ModernTCN.py ------> models/ModernTCN_layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "# forecast task head\n",
    "class Flatten_Head(nn.Module):\n",
    "    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n",
    "        super(Flatten_Head, self).__init__()\n",
    "\n",
    "        self.individual = individual\n",
    "        self.n_vars = n_vars\n",
    "\n",
    "        if self.individual:\n",
    "            self.linears = nn.ModuleList()\n",
    "            self.dropouts = nn.ModuleList()\n",
    "            self.flattens = nn.ModuleList()\n",
    "            for i in range(self.n_vars):\n",
    "                self.flattens.append(nn.Flatten(start_dim=-2))\n",
    "                self.linears.append(nn.Linear(nf, target_window))\n",
    "                self.dropouts.append(nn.Dropout(head_dropout))\n",
    "        else:\n",
    "            self.flatten = nn.Flatten(start_dim=-2)\n",
    "            self.linear = nn.Linear(nf, target_window)\n",
    "            self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n",
    "        if self.individual:\n",
    "            x_out = []\n",
    "            for i in range(self.n_vars):\n",
    "                z = self.flattens[i](x[:, i, :, :])  # z: [bs x d_model * patch_num]\n",
    "                z = self.linears[i](z)  # z: [bs x target_window]\n",
    "                z = self.dropouts[i](z)\n",
    "                x_out.append(z)\n",
    "            x = torch.stack(x_out, dim=1)  # x: [bs x nvars x target_window]\n",
    "        else:\n",
    "            x = self.flatten(x)\n",
    "            x = self.linear(x)\n",
    "            x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. run.py --------> exp/exp_classification.py --------> exp_basics.py ---------> models/ModernTCN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.norm = nn.Layernorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, M, D, N = x.shape\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = x.reshape(B * M, N, D)\n",
    "        x = self.norm(\n",
    "            x)\n",
    "        x = x.reshape(B, M, N, D)\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        return x\n",
    "\n",
    "def get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias):\n",
    "    return nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                     padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "\n",
    "\n",
    "def get_bn(channels):\n",
    "    return nn.BatchNorm1d(channels)\n",
    "\n",
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups, dilation=1,bias=False):\n",
    "    if padding is None:\n",
    "        padding = kernel_size // 2\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv', get_conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                         stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias))\n",
    "    result.add_module('bn', get_bn(out_channels))\n",
    "    return result\n",
    "\n",
    "def fuse_bn(conv, bn):\n",
    "\n",
    "    kernel = conv.weight\n",
    "    running_mean = bn.running_mean\n",
    "    running_var = bn.running_var\n",
    "    gamma = bn.weight\n",
    "    beta = bn.bias\n",
    "    eps = bn.eps\n",
    "    std = (running_var + eps).sqrt()\n",
    "    t = (gamma / std).reshape(-1, 1, 1)\n",
    "    return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "class ReparamLargeKernelConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride, groups,\n",
    "                 small_kernel,\n",
    "                 small_kernel_merged=False, nvars=7):\n",
    "        super(ReparamLargeKernelConv, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.small_kernel = small_kernel\n",
    "        # We assume the conv does not change the feature map size, so padding = k//2. Otherwise, you may configure padding as you wish, and change the padding of small_conv accordingly.\n",
    "        padding = kernel_size // 2\n",
    "        if small_kernel_merged:\n",
    "            self.lkb_reparam = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                         stride=stride, padding=padding, dilation=1, groups=groups, bias=True)\n",
    "        else:\n",
    "            self.lkb_origin = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        stride=stride, padding=padding, dilation=1, groups=groups,bias=False)\n",
    "            if small_kernel is not None:\n",
    "                assert small_kernel <= kernel_size, 'The kernel size for re-param cannot be larger than the large kernel!'\n",
    "                self.small_conv = conv_bn(in_channels=in_channels, out_channels=out_channels,\n",
    "                                            kernel_size=small_kernel,\n",
    "                                            stride=stride, padding=small_kernel // 2, groups=groups, dilation=1,bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        if hasattr(self, 'lkb_reparam'):\n",
    "            out = self.lkb_reparam(inputs)\n",
    "        else:\n",
    "            out = self.lkb_origin(inputs)\n",
    "            if hasattr(self, 'small_conv'):\n",
    "                out += self.small_conv(inputs)\n",
    "        return out\n",
    "\n",
    "    def PaddingTwoEdge1d(self,x,pad_length_left,pad_length_right,pad_values=0):\n",
    "\n",
    "        D_out,D_in,ks=x.shape\n",
    "        if pad_values ==0:\n",
    "            pad_left = torch.zeros(D_out,D_in,pad_length_left)\n",
    "            pad_right = torch.zeros(D_out,D_in,pad_length_right)\n",
    "        else:\n",
    "            pad_left = torch.ones(D_out, D_in, pad_length_left) * pad_values\n",
    "            pad_right = torch.ones(D_out, D_in, pad_length_right) * pad_values\n",
    "        x = torch.cat([pad_left,x],dims=-1)\n",
    "        x = torch.cat([x,pad_right],dims=-1)\n",
    "        return x\n",
    "\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        eq_k, eq_b = fuse_bn(self.lkb_origin.conv, self.lkb_origin.bn)\n",
    "        if hasattr(self, 'small_conv'):\n",
    "            small_k, small_b = fuse_bn(self.small_conv.conv, self.small_conv.bn)\n",
    "            eq_b += small_b\n",
    "            eq_k += self.PaddingTwoEdge1d(small_k, (self.kernel_size - self.small_kernel) // 2,\n",
    "                                          (self.kernel_size - self.small_kernel) // 2, 0)\n",
    "        return eq_k, eq_b\n",
    "\n",
    "    def merge_kernel(self):\n",
    "        eq_k, eq_b = self.get_equivalent_kernel_bias()\n",
    "        self.lkb_reparam = nn.Conv1d(in_channels=self.lkb_origin.conv.in_channels,\n",
    "                                     out_channels=self.lkb_origin.conv.out_channels,\n",
    "                                     kernel_size=self.lkb_origin.conv.kernel_size, stride=self.lkb_origin.conv.stride,\n",
    "                                     padding=self.lkb_origin.conv.padding, dilation=self.lkb_origin.conv.dilation,\n",
    "                                     groups=self.lkb_origin.conv.groups, bias=True)\n",
    "        self.lkb_reparam.weight.data = eq_k\n",
    "        self.lkb_reparam.bias.data = eq_b\n",
    "        self.__delattr__('lkb_origin')\n",
    "        if hasattr(self, 'small_conv'):\n",
    "            self.__delattr__('small_conv')\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, large_size, small_size, dmodel, dff, nvars, small_kernel_merged=False, drop=0.1):\n",
    "\n",
    "        super(Block, self).__init__()\n",
    "        self.dw = ReparamLargeKernelConv(in_channels=nvars * dmodel, out_channels=nvars * dmodel,\n",
    "                                         kernel_size=large_size, stride=1, groups=nvars * dmodel,\n",
    "                                         small_kernel=small_size, small_kernel_merged=small_kernel_merged, nvars=nvars)\n",
    "        self.norm = nn.BatchNorm1d(dmodel)\n",
    "\n",
    "        #convffn1\n",
    "        self.ffn1pw1 = nn.Conv1d(in_channels=nvars * dmodel, out_channels=nvars * dff, kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=1, groups=nvars)\n",
    "        self.ffn1act = nn.GELU()\n",
    "        self.ffn1pw2 = nn.Conv1d(in_channels=nvars * dff, out_channels=nvars * dmodel, kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=1, groups=nvars)\n",
    "        self.ffn1drop1 = nn.Dropout(drop)\n",
    "        self.ffn1drop2 = nn.Dropout(drop)\n",
    "\n",
    "        #convffn2\n",
    "        self.ffn2pw1 = nn.Conv1d(in_channels=nvars * dmodel, out_channels=nvars * dff, kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=1, groups=dmodel)\n",
    "        self.ffn2act = nn.GELU()\n",
    "        self.ffn2pw2 = nn.Conv1d(in_channels=nvars * dff, out_channels=nvars * dmodel, kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=1, groups=dmodel)\n",
    "        self.ffn2drop1 = nn.Dropout(drop)\n",
    "        self.ffn2drop2 = nn.Dropout(drop)\n",
    "\n",
    "        self.ffn_ratio = dff//dmodel\n",
    "    def forward(self,x):\n",
    "\n",
    "        input = x\n",
    "        B, M, D, N = x.shape\n",
    "        x = x.reshape(B,M*D,N)\n",
    "        x = self.dw(x)\n",
    "        x = x.reshape(B,M,D,N)\n",
    "        x = x.reshape(B*M,D,N)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, M, D, N)\n",
    "        x = x.reshape(B, M * D, N)\n",
    "\n",
    "        x = self.ffn1drop1(self.ffn1pw1(x))\n",
    "        x = self.ffn1act(x)\n",
    "        x = self.ffn1drop2(self.ffn1pw2(x))\n",
    "        x = x.reshape(B, M, D, N)\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(B, D * M, N)\n",
    "        x = self.ffn2drop1(self.ffn2pw1(x))\n",
    "        x = self.ffn2act(x)\n",
    "        x = self.ffn2drop2(self.ffn2pw2(x))\n",
    "        x = x.reshape(B, D, M, N)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "\n",
    "        x = input + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Stage(nn.Module):\n",
    "    def __init__(self, ffn_ratio, num_blocks, large_size, small_size, dmodel, dw_model, nvars,\n",
    "                 small_kernel_merged=False, drop=0.1):\n",
    "\n",
    "        super(Stage, self).__init__()\n",
    "        d_ffn = dmodel * ffn_ratio\n",
    "        blks = []\n",
    "        for i in range(num_blocks):\n",
    "            blk = Block(large_size=large_size, small_size=small_size, dmodel=dmodel, dff=d_ffn, nvars=nvars, small_kernel_merged=small_kernel_merged, drop=drop)\n",
    "            blks.append(blk)\n",
    "\n",
    "        self.blocks = nn.ModuleList(blks)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ModernTCN(nn.Module):\n",
    "    def __init__(self,task_name,patch_size,patch_stride, stem_ratio, downsample_ratio, ffn_ratio, num_blocks, large_size, small_size, dims, dw_dims,\n",
    "                 nvars, small_kernel_merged=False, backbone_dropout=0.1, head_dropout=0.1, use_multi_scale=True, revin=True, affine=True,\n",
    "                 subtract_last=False, freq=None, seq_len=512, c_in=7, individual=False, target_window=96, class_drop=0.,class_num = 10):\n",
    "\n",
    "        super(ModernTCN, self).__init__()\n",
    "\n",
    "        self.task_name = task_name\n",
    "        self.class_drop = class_drop\n",
    "        self.class_num = class_num\n",
    "\n",
    "\n",
    "        # RevIN\n",
    "        self.revin = revin\n",
    "        if self.revin: self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n",
    "\n",
    "        # stem layer & down sampling layers\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv1d(1, dims[0], kernel_size=patch_size, stride=patch_stride),\n",
    "            nn.BatchNorm1d(dims[0])\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "\n",
    "        self.num_stage = len(num_blocks)\n",
    "        if self.num_stage > 1:\n",
    "            for i in range(self.num_stage - 1):\n",
    "                downsample_layer = nn.Sequential(\n",
    "                    nn.BatchNorm1d(dims[i]),\n",
    "                    nn.Conv1d(dims[i], dims[i + 1], kernel_size=downsample_ratio, stride=downsample_ratio),\n",
    "                )\n",
    "                self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_stride = patch_stride\n",
    "        self.downsample_ratio = downsample_ratio\n",
    "\n",
    "        # backbone\n",
    "        self.num_stage = len(num_blocks)\n",
    "        self.stages = nn.ModuleList()\n",
    "        for stage_idx in range(self.num_stage):\n",
    "            layer = Stage(ffn_ratio, num_blocks[stage_idx], large_size[stage_idx], small_size[stage_idx], dmodel=dims[stage_idx],\n",
    "                          dw_model=dw_dims[stage_idx], nvars=nvars, small_kernel_merged=small_kernel_merged, drop=backbone_dropout)\n",
    "            self.stages.append(layer)\n",
    "\n",
    "\n",
    "        # head\n",
    "        patch_num = seq_len // patch_stride\n",
    "        self.n_vars = c_in\n",
    "        self.individual = individual\n",
    "        d_model = dims[self.num_stage-1]\n",
    "\n",
    "\n",
    "        if use_multi_scale:\n",
    "            self.head_nf = d_model * patch_num\n",
    "            self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window,\n",
    "                                     head_dropout=head_dropout)\n",
    "        else:\n",
    "            if patch_num % pow(downsample_ratio,(self.num_stage - 1)) == 0:\n",
    "                self.head_nf = d_model * patch_num // pow(downsample_ratio,(self.num_stage - 1))\n",
    "            else:\n",
    "                self.head_nf = d_model * (patch_num // pow(downsample_ratio, (self.num_stage - 1))+1)\n",
    "\n",
    "\n",
    "            self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window,\n",
    "                                     head_dropout=head_dropout)\n",
    "\n",
    "        if self.task_name == 'classification':\n",
    "            self.act_class = F.gelu\n",
    "            self.class_dropout = nn.Dropout(self.class_drop)\n",
    "\n",
    "            self.head_class = nn.Linear(self.n_vars[0]*self.head_nf,self.class_num)\n",
    "\n",
    "\n",
    "    def forward_feature(self, x, te=None):\n",
    "\n",
    "        B,M,L=x.shape\n",
    "\n",
    "        x = x.unsqueeze(-2)\n",
    "\n",
    "        for i in range(self.num_stage):\n",
    "            B, M, D, N = x.shape\n",
    "            x = x.reshape(B * M, D, N)\n",
    "            if i==0:\n",
    "                if self.patch_size != self.patch_stride:\n",
    "                    # stem layer padding\n",
    "                    pad_len = self.patch_size - self.patch_stride\n",
    "                    pad = x[:,:,-1:].repeat(1,1,pad_len)\n",
    "                    x = torch.cat([x,pad],dim=-1)\n",
    "            else:\n",
    "                if N % self.downsample_ratio != 0:\n",
    "                    pad_len = self.downsample_ratio - (N % self.downsample_ratio)\n",
    "                    x = torch.cat([x, x[:, :, -pad_len:]],dim=-1)\n",
    "            x = self.downsample_layers[i](x)\n",
    "            _, D_, N_ = x.shape\n",
    "            x = x.reshape(B, M, D_, N_)\n",
    "            x = self.stages[i](x)\n",
    "        return x\n",
    "\n",
    "    def classification(self,x):\n",
    "\n",
    "        x =  self.forward_feature(x,te=None)\n",
    "        x = self.act_class(x)\n",
    "        x = self.class_dropout(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.head_class(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x, te=None):\n",
    "\n",
    "        if self.task_name == 'classification':\n",
    "            x = self.classification(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def structural_reparam(self):\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'merge_kernel'):\n",
    "                m.merge_kernel()\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        # hyper param\n",
    "        self.task_name = configs.task_name\n",
    "        self.stem_ratio = configs.stem_ratio\n",
    "        self.downsample_ratio = configs.downsample_ratio\n",
    "        self.ffn_ratio = configs.ffn_ratio\n",
    "        self.num_blocks = configs.num_blocks\n",
    "        self.large_size = configs.large_size\n",
    "        self.small_size = configs.small_size\n",
    "        self.dims = configs.dims\n",
    "        self.dw_dims = configs.dw_dims\n",
    "\n",
    "        self.nvars = configs.enc_in\n",
    "        self.small_kernel_merged = configs.small_kernel_merged\n",
    "        self.drop_backbone = configs.dropout\n",
    "        self.drop_head = configs.head_dropout\n",
    "        self.use_multi_scale = configs.use_multi_scale\n",
    "        self.revin = configs.revin\n",
    "        self.affine = configs.affine\n",
    "        self.subtract_last = configs.subtract_last\n",
    "\n",
    "        self.freq = configs.freq\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.c_in = self.nvars,\n",
    "        self.individual = configs.individual\n",
    "        self.target_window = configs.pred_len\n",
    "\n",
    "        self.kernel_size = configs.kernel_size\n",
    "        self.patch_size = configs.patch_size\n",
    "        self.patch_stride = configs.patch_stride\n",
    "\n",
    "        #classification\n",
    "        self.class_dropout = configs.class_dropout\n",
    "        self.class_num = configs.num_class\n",
    "\n",
    "\n",
    "        # decomp\n",
    "        self.decomposition = configs.decomposition\n",
    "\n",
    "\n",
    "        self.model = ModernTCN(task_name=self.task_name,patch_size=self.patch_size, patch_stride=self.patch_stride, stem_ratio=self.stem_ratio,\n",
    "                           downsample_ratio=self.downsample_ratio, ffn_ratio=self.ffn_ratio, num_blocks=self.num_blocks,\n",
    "                           large_size=self.large_size, small_size=self.small_size, dims=self.dims, dw_dims=self.dw_dims,\n",
    "                           nvars=self.nvars, small_kernel_merged=self.small_kernel_merged,\n",
    "                           backbone_dropout=self.drop_backbone, head_dropout=self.drop_head,\n",
    "                           use_multi_scale=self.use_multi_scale, revin=self.revin, affine=self.affine,\n",
    "                           subtract_last=self.subtract_last, freq=self.freq, seq_len=self.seq_len, c_in=self.c_in,\n",
    "                           individual=self.individual, target_window=self.target_window,\n",
    "                            class_drop = self.class_dropout, class_num = self.class_num)\n",
    "\n",
    "    def forward(self, x, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        te = None\n",
    "        x = self.model(x, te)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. run.py --------> exp/exp_classification.py --------> exp_basics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_Basic(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.model_dict = {\n",
    "\n",
    "            'ModernTCN':ModernTCN,\n",
    "\n",
    "\n",
    "        }\n",
    "        self.device = self._acquire_device()\n",
    "        self.model = self._build_model().to(self.device)\n",
    "\n",
    "    def _build_model(self):\n",
    "        raise NotImplementedError\n",
    "        return None\n",
    "\n",
    "    def _acquire_device(self):\n",
    "        if self.args.use_gpu:\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(\n",
    "                self.args.gpu) if not self.args.use_multi_gpu else self.args.devices\n",
    "            device = torch.device('cuda:{}'.format(self.args.gpu))\n",
    "            print('Use GPU: cuda:{}'.format(self.args.gpu))\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print('Use CPU')\n",
    "        return device\n",
    "\n",
    "    def _get_data(self):\n",
    "        pass\n",
    "\n",
    "    def vali(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. run.py --------> exp/exp_classification.py ----------> utils/tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, scheduler, epoch, args, printout=True):\n",
    "    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n",
    "    if args.lradj == 'type1':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "    elif args.lradj == 'type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "    elif args.lradj == 'type3':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 3 else args.learning_rate * (0.9 ** ((epoch - 3) // 1))}\n",
    "    elif args.lradj == 'type4':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 20 else args.learning_rate * (0.5 ** ((epoch // 20) // 1))}\n",
    "    elif args.lradj == 'type5':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate * (0.5 ** ((epoch // 10) // 1))}\n",
    "    elif args.lradj == 'type6':\n",
    "        lr_adjust = {20: args.learning_rate * 0.5 , 40: args.learning_rate * 0.01, 60:args.learning_rate * 0.01,8:args.learning_rate * 0.01,100:args.learning_rate * 0.01 }\n",
    "    elif args.lradj == 'constant':\n",
    "        lr_adjust = {epoch: args.learning_rate}\n",
    "    elif args.lradj == '3':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '4':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 15 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '5':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 25 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '6':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 5 else args.learning_rate*0.1}  \n",
    "    elif args.lradj == 'TST':\n",
    "        lr_adjust = {epoch: scheduler.get_last_lr()[0]}\n",
    "    \n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        if printout: print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "class StandardScaler():\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n",
    "\n",
    "\n",
    "def visual(true, preds=None, name='./pic/test.pdf'):\n",
    "    \"\"\"\n",
    "    Results visualization\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(true, label='GroundTruth', linewidth=2)\n",
    "    if preds is not None:\n",
    "        plt.plot(preds, label='Prediction', linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.savefig(name, bbox_inches='tight')\n",
    "\n",
    "def test_params_flop(model,x_shape):\n",
    "    \"\"\"\n",
    "    If you want to thest former's flop, you need to give default value to inputs in model.forward(), the following code can only pass one argument to forward()\n",
    "    \"\"\"\n",
    "    model_params = 0\n",
    "    for parameter in model.parameters():\n",
    "        model_params += parameter.numel()\n",
    "        print('INFO: Trainable parameter count: {:.2f}M'.format(model_params / 1000000.0))\n",
    "    from ptflops import get_model_complexity_info    \n",
    "    with torch.cuda.device(0):\n",
    "        macs, params = get_model_complexity_info(model.cuda(), x_shape, as_strings=True, print_per_layer_stat=True)\n",
    "        # print('Flops:' + flops)\n",
    "        # print('Params:' + params)\n",
    "        print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "def adjustment(gt, pred):\n",
    "    anomaly_state = False\n",
    "    for i in range(len(gt)):\n",
    "        if gt[i] == 1 and pred[i] == 1 and not anomaly_state:\n",
    "            anomaly_state = True\n",
    "            for j in range(i, 0, -1):\n",
    "                if gt[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if pred[j] == 0:\n",
    "                        pred[j] = 1\n",
    "            for j in range(i, len(gt)):\n",
    "                if gt[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if pred[j] == 0:\n",
    "                        pred[j] = 1\n",
    "        elif gt[i] == 0:\n",
    "            anomaly_state = False\n",
    "        if anomaly_state:\n",
    "            pred[i] = 1\n",
    "    return gt, pred\n",
    "\n",
    "def cal_accuracy(y_pred, y_true):\n",
    "    return np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. run.py --------> exp/exp_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp_Classification(Exp_Basic):\n",
    "    def __init__(self, args):\n",
    "        super(Exp_Classification, self).__init__(args)\n",
    "\n",
    "    def _build_model(self):\n",
    "        # model input depends on data\n",
    "        train_data, train_loader = self._get_data(flag='TRAIN')\n",
    "        test_data, test_loader = self._get_data(flag='TEST')\n",
    "        self.args.seq_len = max(train_data.max_seq_len, test_data.max_seq_len)\n",
    "        self.args.pred_len = 0\n",
    "        self.args.enc_in = train_data.feature_df.shape[1]\n",
    "        self.args.num_class = len(train_data.class_names)\n",
    "        # model init\n",
    "        model = self.model_dict[self.args.model].Model(self.args).float()\n",
    "        if self.args.use_multi_gpu and self.args.use_gpu:\n",
    "            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n",
    "        return model\n",
    "\n",
    "    def _get_data(self, flag):\n",
    "        data_set, data_loader = data_provider(self.args, flag)\n",
    "        return data_set, data_loader\n",
    "\n",
    "    def _select_optimizer(self):\n",
    "        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        return model_optim\n",
    "\n",
    "    def _select_criterion(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        return criterion\n",
    "\n",
    "    def vali(self, vali_data, vali_loader, criterion):\n",
    "        total_loss = []\n",
    "        preds = []\n",
    "        trues = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, label, padding_mask) in enumerate(vali_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                padding_mask = padding_mask.float().to(self.device)\n",
    "                label = label.to(self.device)\n",
    "\n",
    "                outputs = self.model(batch_x, padding_mask, None, None)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                loss = criterion(pred, label.long().squeeze().cpu())\n",
    "                total_loss.append(loss)\n",
    "\n",
    "                preds.append(outputs.detach())\n",
    "                trues.append(label)\n",
    "\n",
    "        total_loss = np.average(total_loss)\n",
    "\n",
    "        preds = torch.cat(preds, 0)\n",
    "        trues = torch.cat(trues, 0)\n",
    "        probs = torch.nn.functional.softmax(preds)  # (total_samples, num_classes) est. prob. for each class and sample\n",
    "        predictions = torch.argmax(probs, dim=1).cpu().numpy()  # (total_samples,) int class index for each sample\n",
    "        trues = trues.flatten().cpu().numpy()\n",
    "        accuracy = cal_accuracy(predictions, trues)\n",
    "\n",
    "        self.model.train()\n",
    "        return total_loss, accuracy\n",
    "\n",
    "    def train(self, setting):\n",
    "        train_data, train_loader = self._get_data(flag='TRAIN')\n",
    "        vali_data, vali_loader = self._get_data(flag='TEST')\n",
    "        test_data, test_loader = self._get_data(flag='TEST')\n",
    "\n",
    "        path = os.path.join(self.args.checkpoints, setting)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        time_now = time.time()\n",
    "\n",
    "        train_steps = len(train_loader)\n",
    "        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\n",
    "\n",
    "        model_optim = self._select_optimizer()\n",
    "        criterion = self._select_criterion()\n",
    "\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                            steps_per_epoch=train_steps,\n",
    "                                            pct_start=self.args.pct_start,\n",
    "                                            epochs=self.args.train_epochs,\n",
    "                                            max_lr=self.args.learning_rate)\n",
    "\n",
    "        for epoch in range(self.args.train_epochs):\n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "\n",
    "            self.model.train()\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            for i, (batch_x, label, padding_mask) in enumerate(train_loader):\n",
    "                iter_count += 1\n",
    "                model_optim.zero_grad()\n",
    "\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                padding_mask = padding_mask.float().to(self.device)\n",
    "                label = label.to(self.device)\n",
    "\n",
    "                outputs = self.model(batch_x, padding_mask, None, None)\n",
    "                loss = criterion(outputs, label.long().squeeze(-1))\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n",
    "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=4.0)\n",
    "                model_optim.step()\n",
    "\n",
    "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(train_loss)\n",
    "            vali_loss, val_accuracy = self.vali(vali_data, vali_loader, criterion)\n",
    "            test_loss, test_accuracy = self.vali(test_data, test_loader, criterion)\n",
    "\n",
    "            print(\n",
    "                \"Epoch: {0}, Steps: {1} | Train Loss: {2:.3f} Vali Loss: {3:.3f} Vali Acc: {4:.3f} Test Loss: {5:.3f} Test Acc: {6:.3f}\"\n",
    "                .format(epoch + 1, train_steps, train_loss, vali_loss, val_accuracy, test_loss, test_accuracy))\n",
    "            early_stopping(-val_accuracy, self.model, path)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                adjust_learning_rate(model_optim,  scheduler, epoch + 1, self.args)\n",
    "\n",
    "        best_model_path = path + '/' + 'checkpoint.pth'\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def test(self, setting, test=0):\n",
    "        test_data, test_loader = self._get_data(flag='TEST')\n",
    "        if test:\n",
    "            print('loading model')\n",
    "            self.model.load_state_dict(torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint.pth')))\n",
    "\n",
    "        preds = []\n",
    "        trues = []\n",
    "        folder_path = './test_results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, label, padding_mask) in enumerate(test_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                padding_mask = padding_mask.float().to(self.device)\n",
    "                label = label.to(self.device)\n",
    "\n",
    "                outputs = self.model(batch_x, padding_mask, None, None)\n",
    "\n",
    "                preds.append(outputs.detach())\n",
    "                trues.append(label)\n",
    "\n",
    "        preds = torch.cat(preds, 0)\n",
    "        trues = torch.cat(trues, 0)\n",
    "        print('test shape:', preds.shape, trues.shape)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(preds)  # (total_samples, num_classes) est. prob. for each class and sample\n",
    "        predictions = torch.argmax(probs, dim=1).cpu().numpy()  # (total_samples,) int class index for each sample\n",
    "        trues = trues.flatten().cpu().numpy()\n",
    "        accuracy = cal_accuracy(predictions, trues)\n",
    "\n",
    "        # result save\n",
    "        folder_path = './results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        print('accuracy:{}'.format(accuracy))\n",
    "        f = open(\"result_classification.txt\", 'a')\n",
    "        f.write(setting + \"  \\n\")\n",
    "        f.write('accuracy:{}'.format(accuracy))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. run.py - Without argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "args = {\n",
    "\n",
    "    # random seed\n",
    "    'random_seed': 2021,  # random seed\n",
    "    \n",
    "    # basic config\n",
    "    'is_training': 1,  # status\n",
    "    'model_id': 'test',  # model id\n",
    "    'model': 'ModernTCN',  # model name, options: [ModernTCN]\n",
    "    \n",
    "    # data loader\n",
    "    'data': 'ETTm1',  # dataset type\n",
    "    'root_path': './data/ETT/',  # root path of the data file\n",
    "    'data_path': 'ETTh1.csv',  # data file\n",
    "    'features': 'M',  # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "    'target': 'OT',  # target feature in S or MS task\n",
    "    'freq': 'h',  # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "    'checkpoints': './checkpoints/',  # location of model checkpoints\n",
    "    'embed': 'timeF',  # time features encoding, options:[timeF, fixed, learned]\n",
    "    \n",
    "    # forecasting task\n",
    "    'seq_len': 96,  # input sequence length\n",
    "    'label_len': 48,  # start token length\n",
    "    'pred_len': 96,  # prediction sequence length\n",
    "    \n",
    "    #ModernTCN\n",
    "    'stem_ratio': 6,  # stem ratio\n",
    "    'downsample_ratio': 2,  # downsample_ratio\n",
    "    'ffn_ratio': 2,  # ffn_ratio\n",
    "    'patch_size': 16,  # the patch size\n",
    "    'patch_stride': 8,  # the patch stride\n",
    "    \n",
    "    'num_blocks': [1, 1, 1, 1],  # num_blocks in each stage\n",
    "    'large_size': [31, 29, 27, 13],  # big kernel size\n",
    "    'small_size': [5, 5, 5, 5],  # small kernel size for structral reparam\n",
    "    'dims': [256, 256, 256, 256],  # dmodels in each stage\n",
    "    'dw_dims': [256, 256, 256, 256],  # dw dims in dw conv in each stage\n",
    "    \n",
    "    'small_kernel_merged': False,  # small_kernel has already merged or not\n",
    "    'call_structural_reparam': False,  # structural_reparam after training\n",
    "    'use_multi_scale': True,  # use_multi_scale fusion\n",
    "    \n",
    "    # PatchTST\n",
    "    'fc_dropout': 0.05,  # fully connected dropout\n",
    "    'head_dropout': 0.0,  # head dropout\n",
    "    'patch_len': 16,  # patch length\n",
    "    'stride': 8,  # stride\n",
    "    'padding_patch': 'end',  # None: None; end: padding on the end\n",
    "    'revin': 1,  # RevIN; True 1 False 0\n",
    "    'affine': 0,  # RevIN-affine; True 1 False 0\n",
    "    'subtract_last': 0,  # 0: subtract mean; 1: subtract last\n",
    "    'decomposition': 0,  # decomposition; True 1 False 0\n",
    "    'kernel_size': 25,  # decomposition-kernel\n",
    "    'individual': 0,  # individual head; True 1 False 0\n",
    "    \n",
    "    # Formers\n",
    "    'embed_type': 0,  # 0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding\n",
    "    'enc_in': 7,  # encoder input size\n",
    "    'dec_in': 7,  # decoder input size\n",
    "    'c_out': 7,  # output size\n",
    "    'd_model': 512,  # dimension of model\n",
    "    'n_heads': 8,  # num of heads\n",
    "    'e_layers': 2,  # num of encoder layers\n",
    "    'd_layers': 1,  # num of decoder layers\n",
    "    'd_ff': 2048,  # dimension of fcn\n",
    "    'moving_avg': 25,  # window size of moving average\n",
    "    'factor': 1,  # attn factor\n",
    "    'distil': True,  # whether to use distilling in encoder, using this argument means not using distilling\n",
    "    'dropout': 0.05,  # dropout\n",
    "    'activation': 'gelu',  # activation\n",
    "    'output_attention': False,  # whether to output attention in ecoder\n",
    "    'do_predict': False,  # whether to predict unseen future data\n",
    "    \n",
    "    # optimization\n",
    "    'num_workers': 0,  # data loader num workers\n",
    "    'itr': 2,  # experiments times\n",
    "    'train_epochs': 100,  # train epochs\n",
    "    'batch_size': 128,  # batch size of train input data\n",
    "    'patience': 100,  # early stopping patience\n",
    "    'learning_rate': 0.0001,  # optimizer learning rate\n",
    "    'des': 'test',  # exp description\n",
    "    'loss': 'mse',  # loss function\n",
    "    'lradj': 'type1',  # adjust learning rate\n",
    "    'pct_start': 0.3,  # pct_start\n",
    "    'use_amp': False,  # use automatic mixed precision training\n",
    "    \n",
    "    # GPU\n",
    "    'use_gpu': True,  # use gpu\n",
    "    'gpu': 0,  # gpu\n",
    "    'use_multi_gpu': False,  # use multiple gpus\n",
    "    'devices': '0,1,2,3',  # device ids of multile gpus\n",
    "    'test_flop': False,  # See utils/tools for usage\n",
    "    \n",
    "    #multi task\n",
    "    'task_name': 'classification',  # task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]\n",
    "    \n",
    "    # inputation task\n",
    "    'mask_rate': 0.25,  # mask ratio\n",
    "    \n",
    "    # anomaly detection task\n",
    "    'anomaly_ratio': 0.25,  # prior anomaly ratio (%)\n",
    "    \n",
    "    # classfication task\n",
    "    'class_dropout': 0.05  # classfication dropout\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'random_seed': 2021, 'is_training': 1, 'model_id': 'test', 'model': 'ModernTCN', 'data': 'ETTm1', 'root_path': './data/ETT/', 'data_path': 'ETTh1.csv', 'features': 'M', 'target': 'OT', 'freq': 'h', 'checkpoints': './checkpoints/', 'embed': 'timeF', 'seq_len': 96, 'label_len': 48, 'pred_len': 96, 'stem_ratio': 6, 'downsample_ratio': 2, 'ffn_ratio': 2, 'patch_size': 16, 'patch_stride': 8, 'num_blocks': [1, 1, 1, 1], 'large_size': [31, 29, 27, 13], 'small_size': [5, 5, 5, 5], 'dims': [256, 256, 256, 256], 'dw_dims': [256, 256, 256, 256], 'small_kernel_merged': False, 'call_structural_reparam': False, 'use_multi_scale': True, 'fc_dropout': 0.05, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'individual': 0, 'embed_type': 0, 'enc_in': 7, 'dec_in': 7, 'c_out': 7, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 1, 'distil': True, 'dropout': 0.05, 'activation': 'gelu', 'output_attention': False, 'do_predict': False, 'num_workers': 0, 'itr': 2, 'train_epochs': 100, 'batch_size': 128, 'patience': 100, 'learning_rate': 0.0001, 'des': 'test', 'loss': 'mse', 'lradj': 'type1', 'pct_start': 0.3, 'use_amp': False, 'use_gpu': False, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'test_flop': False, 'task_name': 'classification', 'mask_rate': 0.25, 'anomaly_ratio': 0.25, 'class_dropout': 0.05}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'use_gpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 45\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitr\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     26\u001b[0m     setting \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ft\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_sl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_pl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dim\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_nb\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_lk\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_sk\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ffr\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ps\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_str\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_multi\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_merged\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     27\u001b[0m         args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     28\u001b[0m         args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m         args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdes\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     43\u001b[0m         ii)\n\u001b[0;32m---> 45\u001b[0m     exp \u001b[38;5;241m=\u001b[39m \u001b[43mExp\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# set experiments\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[1;32m     47\u001b[0m     exp\u001b[38;5;241m.\u001b[39mtrain(setting)\n",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m, in \u001b[0;36mExp_Classification.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mExp_Classification\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 10\u001b[0m, in \u001b[0;36mExp_Basic.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModernTCN\u001b[39m\u001b[38;5;124m'\u001b[39m:ModernTCN,\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_model()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[49], line 18\u001b[0m, in \u001b[0;36mExp_Basic._acquire_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire_device\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_gpu\u001b[49m:\n\u001b[1;32m     19\u001b[0m         os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgpu) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_multi_gpu \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevices\n\u001b[1;32m     21\u001b[0m         device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgpu))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'use_gpu'"
     ]
    }
   ],
   "source": [
    "fix_seed = args['random_seed']\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "args['use_gpu'] = torch.cuda.is_available() and args['use_gpu']\n",
    "\n",
    "if args['use_gpu'] and args['use_multi_gpu']:\n",
    "    args['devices'] = args['devices'].replace(' ', '')\n",
    "    device_ids = args['devices'].split(',')\n",
    "    args['device_ids'] = [int(id_) for id_ in device_ids]\n",
    "    args['gpu'] = args['device_ids'][0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "\n",
    "if args['task_name'] == 'classification':\n",
    "    Exp = Exp_Classification\n",
    "\n",
    "if args['large_size'][0] < 13:\n",
    "    args['small_kernel_merged'] = True\n",
    "\n",
    "if args['is_training']:\n",
    "    for ii in range(args['itr']):\n",
    "        setting = '{}_{}_{}_ft{}_sl{}_pl{}_dim{}_nb{}_lk{}_sk{}_ffr{}_ps{}_str{}_multi{}_merged{}_{}_{}'.format(\n",
    "            args['model_id'],\n",
    "            args['model'],\n",
    "            args['data'],\n",
    "            args['features'],\n",
    "            args['seq_len'],\n",
    "            args['pred_len'],\n",
    "            args['dims'][0],\n",
    "            args['num_blocks'][0],\n",
    "            args['large_size'][0],\n",
    "            args['small_size'][0],\n",
    "            args['ffn_ratio'],\n",
    "            args['patch_size'],\n",
    "            args['patch_stride'],\n",
    "            args['use_multi_scale'],\n",
    "            args['small_kernel_merged'],\n",
    "            args['des'],\n",
    "            ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        if args['do_predict']:\n",
    "            print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.predict(setting, True)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    ii = 0\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(args['model_id'],\n",
    "                                                                                                args['model'],\n",
    "                                                                                                args['data'],\n",
    "                                                                                                args['features'],\n",
    "                                                                                                args['seq_len'],\n",
    "                                                                                                args['label_len'],\n",
    "                                                                                                args['pred_len'],\n",
    "                                                                                                args['d_model'],\n",
    "                                                                                                args['n_heads'],\n",
    "                                                                                                args['e_layers'],\n",
    "                                                                                                args['d_layers'],\n",
    "                                                                                                args['d_ff'],\n",
    "                                                                                                args['factor'],\n",
    "                                                                                                args['embed'],\n",
    "                                                                                                args['distil'],\n",
    "                                                                                                args['des'], ii)\n",
    "\n",
    "    exp = Exp(args)  # set experiments\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting, test=1)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. run.py - With argeparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--random_seed RANDOM_SEED] --is_training\n",
      "                             IS_TRAINING --model_id MODEL_ID --model MODEL\n",
      "                             --data DATA [--root_path ROOT_PATH]\n",
      "                             [--data_path DATA_PATH] [--features FEATURES]\n",
      "                             [--target TARGET] [--freq FREQ]\n",
      "                             [--checkpoints CHECKPOINTS] [--embed EMBED]\n",
      "                             [--seq_len SEQ_LEN] [--label_len LABEL_LEN]\n",
      "                             [--pred_len PRED_LEN] [--stem_ratio STEM_RATIO]\n",
      "                             [--downsample_ratio DOWNSAMPLE_RATIO]\n",
      "                             [--ffn_ratio FFN_RATIO] [--patch_size PATCH_SIZE]\n",
      "                             [--patch_stride PATCH_STRIDE]\n",
      "                             [--num_blocks NUM_BLOCKS [NUM_BLOCKS ...]]\n",
      "                             [--large_size LARGE_SIZE [LARGE_SIZE ...]]\n",
      "                             [--small_size SMALL_SIZE [SMALL_SIZE ...]]\n",
      "                             [--dims DIMS [DIMS ...]]\n",
      "                             [--dw_dims DW_DIMS [DW_DIMS ...]]\n",
      "                             [--small_kernel_merged SMALL_KERNEL_MERGED]\n",
      "                             [--call_structural_reparam CALL_STRUCTURAL_REPARAM]\n",
      "                             [--use_multi_scale USE_MULTI_SCALE]\n",
      "                             [--fc_dropout FC_DROPOUT]\n",
      "                             [--head_dropout HEAD_DROPOUT]\n",
      "                             [--patch_len PATCH_LEN] [--stride STRIDE]\n",
      "                             [--padding_patch PADDING_PATCH] [--revin REVIN]\n",
      "                             [--affine AFFINE] [--subtract_last SUBTRACT_LAST]\n",
      "                             [--decomposition DECOMPOSITION]\n",
      "                             [--kernel_size KERNEL_SIZE]\n",
      "                             [--individual INDIVIDUAL]\n",
      "                             [--embed_type EMBED_TYPE] [--enc_in ENC_IN]\n",
      "                             [--dec_in DEC_IN] [--c_out C_OUT]\n",
      "                             [--d_model D_MODEL] [--n_heads N_HEADS]\n",
      "                             [--e_layers E_LAYERS] [--d_layers D_LAYERS]\n",
      "                             [--d_ff D_FF] [--moving_avg MOVING_AVG]\n",
      "                             [--factor FACTOR] [--distil] [--dropout DROPOUT]\n",
      "                             [--activation ACTIVATION] [--output_attention]\n",
      "                             [--do_predict] [--num_workers NUM_WORKERS]\n",
      "                             [--itr ITR] [--train_epochs TRAIN_EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--patience PATIENCE]\n",
      "                             [--learning_rate LEARNING_RATE] [--des DES]\n",
      "                             [--loss LOSS] [--lradj LRADJ]\n",
      "                             [--pct_start PCT_START] [--use_amp]\n",
      "                             [--use_gpu USE_GPU] [--gpu GPU] [--use_multi_gpu]\n",
      "                             [--devices DEVICES] [--test_flop] --task_name\n",
      "                             TASK_NAME [--mask_rate MASK_RATE]\n",
      "                             [--anomaly_ratio ANOMALY_RATIO]\n",
      "                             [--class_dropout CLASS_DROPOUT]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=/Users/yohanabeysinghe/Library/Jupyter/runtime/kernel-v2-41914SA8U1wIniywL.json could match --features, --freq, --ffn_ratio, --fc_dropout, --factor\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='ModernTCN')\n",
    "\n",
    "# random seed\n",
    "parser.add_argument('--random_seed', type=int, default=2021, help='random seed')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='ModernTCN',\n",
    "                    help='model name, options: [ModernTCN]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "\n",
    "#ModernTCN\n",
    "parser.add_argument('--stem_ratio', type=int, default=6, help='stem ratio')\n",
    "parser.add_argument('--downsample_ratio', type=int, default=2, help='downsample_ratio')\n",
    "parser.add_argument('--ffn_ratio', type=int, default=2, help='ffn_ratio')\n",
    "parser.add_argument('--patch_size', type=int, default=16, help='the patch size')\n",
    "parser.add_argument('--patch_stride', type=int, default=8, help='the patch stride')\n",
    "\n",
    "parser.add_argument('--num_blocks', nargs='+',type=int, default=[1,1,1,1], help='num_blocks in each stage')\n",
    "parser.add_argument('--large_size', nargs='+',type=int, default=[31,29,27,13], help='big kernel size')\n",
    "parser.add_argument('--small_size', nargs='+',type=int, default=[5,5,5,5], help='small kernel size for structral reparam')\n",
    "parser.add_argument('--dims', nargs='+',type=int, default=[256,256,256,256], help='dmodels in each stage')\n",
    "parser.add_argument('--dw_dims', nargs='+',type=int, default=[256,256,256,256], help='dw dims in dw conv in each stage')\n",
    "\n",
    "parser.add_argument('--small_kernel_merged', type=str2bool, default=False, help='small_kernel has already merged or not')\n",
    "parser.add_argument('--call_structural_reparam', type=bool, default=False, help='structural_reparam after training')\n",
    "parser.add_argument('--use_multi_scale', type=str2bool, default=True, help='use_multi_scale fusion')\n",
    "\n",
    "\n",
    "# PatchTST\n",
    "parser.add_argument('--fc_dropout', type=float, default=0.05, help='fully connected dropout')\n",
    "parser.add_argument('--head_dropout', type=float, default=0.0, help='head dropout')\n",
    "parser.add_argument('--patch_len', type=int, default=16, help='patch length')\n",
    "parser.add_argument('--stride', type=int, default=8, help='stride')\n",
    "parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')\n",
    "parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')\n",
    "parser.add_argument('--affine', type=int, default=0, help='RevIN-affine; True 1 False 0')\n",
    "parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')\n",
    "parser.add_argument('--decomposition', type=int, default=0, help='decomposition; True 1 False 0')\n",
    "parser.add_argument('--kernel_size', type=int, default=25, help='decomposition-kernel')\n",
    "parser.add_argument('--individual', type=int, default=0, help='individual head; True 1 False 0')\n",
    "\n",
    "# Formers\n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=0, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=100, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=100, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--pct_start', type=float, default=0.3, help='pct_start')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "#multi task\n",
    "parser.add_argument('--task_name', type=str, required=True, default='long_term_forecast',\n",
    "                        help='task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]')\n",
    "\n",
    "# inputation task\n",
    "parser.add_argument('--mask_rate', type=float, default=0.25, help='mask ratio')\n",
    "\n",
    "# anomaly detection task\n",
    "parser.add_argument('--anomaly_ratio', type=float, default=0.25, help='prior anomaly ratio (%)')\n",
    "\n",
    "# classfication task\n",
    "parser.add_argument('--class_dropout', type=float, default=0.05, help='classfication dropout')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# random seed\n",
    "fix_seed = args.random_seed\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.dvices = args.devices.replace(' ', '')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #Exp = Exp_Main\n",
    "    if args.task_name == 'classification':\n",
    "        Exp = Exp_Classification\n",
    "    if args.large_size[0] < 13:\n",
    "        args.small_kernel_merged = True\n",
    "\n",
    "    if args.is_training:\n",
    "        for ii in range(args.itr):\n",
    "            # setting record of experiments\n",
    "            setting = '{}_{}_{}_ft{}_sl{}_pl{}_dim{}_nb{}_lk{}_sk{}_ffr{}_ps{}_str{}_multi{}_merged{}_{}_{}'.format(\n",
    "                args.model_id,\n",
    "                args.model,\n",
    "                args.data,\n",
    "                args.features,\n",
    "                args.seq_len,\n",
    "                args.pred_len,\n",
    "                args.dims[0],\n",
    "                args.num_blocks[0],\n",
    "                args.large_size[0],\n",
    "                args.small_size[0],\n",
    "                args.ffn_ratio,\n",
    "                args.patch_size,\n",
    "                args.patch_stride,\n",
    "                args.use_multi_scale,\n",
    "                args.small_kernel_merged,\n",
    "                args.des,\n",
    "                ii)\n",
    "\n",
    "            exp = Exp(args)  # set experiments\n",
    "            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "            exp.train(setting)\n",
    "\n",
    "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.test(setting)\n",
    "\n",
    "            if args.do_predict:\n",
    "                print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "                exp.predict(setting, True)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        ii = 0\n",
    "        setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(args.model_id,\n",
    "                                                                                                      args.model,\n",
    "                                                                                                      args.data,\n",
    "                                                                                                      args.features,\n",
    "                                                                                                      args.seq_len,\n",
    "                                                                                                      args.label_len,\n",
    "                                                                                                      args.pred_len,\n",
    "                                                                                                      args.d_model,\n",
    "                                                                                                      args.n_heads,\n",
    "                                                                                                      args.e_layers,\n",
    "                                                                                                      args.d_layers,\n",
    "                                                                                                      args.d_ff,\n",
    "                                                                                                      args.factor,\n",
    "                                                                                                      args.embed,\n",
    "                                                                                                      args.distil,\n",
    "                                                                                                      args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting, test=1)\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. run.py - Without argparse and Without a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 2021  # random seed\n",
    "is_training = 1  # status\n",
    "model_id = 'test'  # model id\n",
    "model = 'ModernTCN'  # model name, options: [ModernTCN]\n",
    "data = 'ETTm1'  # dataset type\n",
    "root_path = './data/ETT/'  # root path of the data file\n",
    "data_path = 'ETTh1.csv'  # data file\n",
    "features = 'M'  # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "target = 'OT'  # target feature in S or MS task\n",
    "freq = 'h'  # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "checkpoints = './checkpoints/'  # location of model checkpoints\n",
    "embed = 'timeF'  # time features encoding, options:[timeF, fixed, learned]\n",
    "seq_len = 96  # input sequence length\n",
    "label_len = 48  # start token length\n",
    "pred_len = 96  # prediction sequence length\n",
    "stem_ratio = 6  # stem ratio\n",
    "downsample_ratio = 2  # downsample_ratio\n",
    "ffn_ratio = 2  # ffn_ratio\n",
    "patch_size = 16  # the patch size\n",
    "patch_stride = 8  # the patch stride\n",
    "num_blocks = [1, 1, 1, 1]  # num_blocks in each stage\n",
    "large_size = [31, 29, 27, 13]  # big kernel size\n",
    "small_size = [5, 5, 5, 5]  # small kernel size for structral reparam\n",
    "dims = [256, 256, 256, 256]  # dmodels in each stage\n",
    "dw_dims = [256, 256, 256, 256]  # dw dims in dw conv in each stage\n",
    "small_kernel_merged = False  # small_kernel has already merged or not\n",
    "call_structural_reparam = False  # structural_reparam after training\n",
    "use_multi_scale = True  # use_multi_scale fusion\n",
    "fc_dropout = 0.05  # fully connected dropout\n",
    "head_dropout = 0.0  # head dropout\n",
    "patch_len = 16  # patch length\n",
    "stride = 8  # stride\n",
    "padding_patch = 'end'  # None: None; end: padding on the end\n",
    "revin = 1  # RevIN; True 1 False 0\n",
    "affine = 0  # RevIN-affine; True 1 False 0\n",
    "subtract_last = 0  # 0: subtract mean; 1: subtract last\n",
    "decomposition = 0  # decomposition; True 1 False 0\n",
    "kernel_size = 25  # decomposition-kernel\n",
    "individual = 0  # individual head; True 1 False 0\n",
    "embed_type = 0  # 0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding\n",
    "enc_in = 7  # encoder input size\n",
    "dec_in = 7  # decoder input size\n",
    "c_out = 7  # output size\n",
    "d_model = 512  # dimension of model\n",
    "n_heads = 8  # num of heads\n",
    "e_layers = 2  # num of encoder layers\n",
    "d_layers = 1  # num of decoder layers\n",
    "d_ff = 2048  # dimension of fcn\n",
    "moving_avg = 25  # window size of moving average\n",
    "factor = 1  # attn factor\n",
    "distil = True  # whether to use distilling in encoder, using this argument means not using distilling\n",
    "dropout = 0.05  # dropout\n",
    "activation = 'gelu'  # activation\n",
    "output_attention = False  # whether to output attention in ecoder\n",
    "do_predict = False  # whether to predict unseen future data\n",
    "num_workers = 0  # data loader num workers\n",
    "itr = 2  # experiments times\n",
    "train_epochs = 100  # train epochs\n",
    "batch_size = 128  # batch size of train input data\n",
    "patience = 100  # early stopping patience\n",
    "learning_rate = 0.0001  # optimizer learning rate\n",
    "des = 'test'  # exp description\n",
    "loss = 'mse'  # loss function\n",
    "lradj = 'type1'  # adjust learning rate\n",
    "pct_start = 0.3  # pct_start\n",
    "use_amp = False  # use automatic mixed precision training\n",
    "use_gpu = True  # use gpu\n",
    "gpu = 0  # gpu\n",
    "use_multi_gpu = False  # use multiple gpus\n",
    "devices = '0,1,2,3'  # device ids of multile gpus\n",
    "test_flop = False  # See utils/tools for usage\n",
    "task_name = 'classification'  # task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]\n",
    "mask_rate = 0.25  # mask ratio\n",
    "anomaly_ratio = 0.25  # prior anomaly ratio (%)\n",
    "class_dropout = 0.05  # classfication dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "random_seed: 2021\n",
      "use_gpu: False\n",
      "use_multi_gpu: False\n",
      "gpu: 0\n",
      "devices: 0,1,2,3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 52\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(itr):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# setting record of experiments\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     setting \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ft\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_sl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_pl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dim\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_nb\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_lk\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_sk\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ffr\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ps\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_str\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_multi\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_merged\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     34\u001b[0m         model_id,\n\u001b[1;32m     35\u001b[0m         model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m         des,\n\u001b[1;32m     50\u001b[0m         ii)\n\u001b[0;32m---> 52\u001b[0m     exp \u001b[38;5;241m=\u001b[39m \u001b[43mExp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# set experiments\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[1;32m     54\u001b[0m     exp\u001b[38;5;241m.\u001b[39mtrain(setting)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'args'"
     ]
    }
   ],
   "source": [
    "# random seed\n",
    "fix_seed = random_seed\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "use_gpu = True if torch.cuda.is_available() and use_gpu else False\n",
    "\n",
    "if use_gpu and use_multi_gpu:\n",
    "    devices = devices.replace(' ', '')\n",
    "    device_ids = devices.split(',')\n",
    "    device_ids = [int(id_) for id_ in device_ids]\n",
    "    gpu = device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(\"random_seed:\", fix_seed)\n",
    "print(\"use_gpu:\", use_gpu)\n",
    "print(\"use_multi_gpu:\", use_multi_gpu)\n",
    "print(\"gpu:\", gpu)\n",
    "print(\"devices:\", devices)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Exp = Exp_Main\n",
    "    if task_name == 'classification':\n",
    "        Exp = Exp_Classification\n",
    "    if large_size[0] < 13:\n",
    "        small_kernel_merged = True\n",
    "\n",
    "    if is_training:\n",
    "        for ii in range(itr):\n",
    "            # setting record of experiments\n",
    "            setting = '{}_{}_{}_ft{}_sl{}_pl{}_dim{}_nb{}_lk{}_sk{}_ffr{}_ps{}_str{}_multi{}_merged{}_{}_{}'.format(\n",
    "                model_id,\n",
    "                model,\n",
    "                data,\n",
    "                features,\n",
    "                seq_len,\n",
    "                pred_len,\n",
    "                dims[0],\n",
    "                num_blocks[0],\n",
    "                large_size[0],\n",
    "                small_size[0],\n",
    "                ffn_ratio,\n",
    "                patch_size,\n",
    "                patch_stride,\n",
    "                use_multi_scale,\n",
    "                small_kernel_merged,\n",
    "                des,\n",
    "                ii)\n",
    "\n",
    "            exp = Exp()  # set experiments\n",
    "            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "            exp.train(setting)\n",
    "\n",
    "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.test(setting)\n",
    "\n",
    "            if do_predict:\n",
    "                print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "                exp.predict(setting, True)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        ii = 0\n",
    "        setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(model_id,\n",
    "                                                                                                      model,\n",
    "                                                                                                      data,\n",
    "                                                                                                      features,\n",
    "                                                                                                      seq_len,\n",
    "                                                                                                      label_len,\n",
    "                                                                                                      pred_len,\n",
    "                                                                                                      d_model,\n",
    "                                                                                                      n_heads,\n",
    "                                                                                                      e_layers,\n",
    "                                                                                                      d_layers,\n",
    "                                                                                                      d_ff,\n",
    "                                                                                                      factor,\n",
    "                                                                                                      embed,\n",
    "                                                                                                      distil,\n",
    "                                                                                                      des, ii)\n",
    "\n",
    "        exp = Exp()  # set experiments\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting, test=1)\n",
    "        torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
